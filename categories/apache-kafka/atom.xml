<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Apache-kafka | Mastering FP and OO with Scala]]></title>
  <link href="http://blog.jaceklaskowski.pl/categories/apache-kafka/atom.xml" rel="self"/>
  <link href="http://blog.jaceklaskowski.pl/"/>
  <updated>2015-07-21T21:20:47+02:00</updated>
  <id>http://blog.jaceklaskowski.pl/</id>
  <author>
    <name><![CDATA[Jacek Laskowski]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Real-time Data Processing Using Apache Kafka and Spark Streaming (and Scala and Sbt)]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/07/20/real-time-data-processing-using-apache-kafka-and-spark-streaming.html"/>
    <updated>2015-07-20T21:02:39+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/07/20/real-time-data-processing-using-apache-kafka-and-spark-streaming</id>
    <content type="html"><![CDATA[It's been a while since I worked with [Spark Streaming](http://spark.apache.org/streaming/). It was back then when I was working for a pet project that ultimately ended up as a Typesafe Activator template [Spark Streaming with Scala and Akka](http://www.typesafe.com/activator/template/spark-streaming-scala-akka) to get people going with the technologies.

Time flies by very quickly and as [the other blog posts](http://blog.jaceklaskowski.pl/2015/07/14/apache-kafka-on-docker.html) [may have showed](http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html) I'm evaluating [Apache Kafka](http://kafka.apache.org/) as a potential messaging and integration platform for my future projects. A lot is happening in so called *big data space* and Apache Kafka fits the bill in many dataflows around me so well. I'm very glad it's mostly all [Scala](http://www.scala-lang.org/) which we all *love* and are spending our time with. Ain't we?

From [Spark Streaming documentation](http://spark.apache.org/docs/latest/streaming-programming-guide.html) (Kafka bolded on purpose):

> Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like **Kafka**, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window.

Since Apache Kafka aims at being **the central hub for real-time streams of data** (see [1.2 Use Cases](http://kafka.apache.org/documentation.html#uses) and [Putting Apache Kafka To Use: A Practical Guide to Building a Stream Data Platform (Part 1)](http://www.confluent.io/blog/stream-data-platform-1/)) I couldn't deny myself the simple pleasure of giving it a go.

Buckle up and ingest *some* data using Apache Kafka and Spark Streaming! You surely *will* love the infrastructure (if you haven't already). Be sure to type fast to see the potential of the platform at your fingertips.

<!-- more -->

## The project (using sbt)

Here is the sbt build file `build.sbt` for the project:

    val sparkVersion = "1.4.1"
    scalaVersion := "2.11.7"

    libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-streaming" % sparkVersion,
      "org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
    )

It uses the latest released versions of **Spark Streaming 1.4.1** and **Scala 2.11.7**.

## Setting up Kafka broker

It assumes you've already installed Apache Kafka. You may want to use Docker (see [Apache Kafka on Docker](http://blog.jaceklaskowski.pl/2015/07/14/apache-kafka-on-docker.html)) or [build Kafka from the sources](http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html). Whatever approach you choose, start Zookeeper and Kafka.

### Starting Zookeeper

I'm using the version of Kafka built from the sources and `./bin/zookeeper-server-start.sh` that comes with it.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/zookeeper-server-start.sh config/zookeeper.properties
    ...
    [2015-07-21 06:51:39,614] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)

### Starting Kafka broker

Once Zookeeper is up and running (in the above case, listening to the port 2181), run a Kafka broker.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-server-start.sh config/server.properties
    ...
    [2015-07-21 06:53:17,051] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT -> EndPoint(192.168.1.9,9092,PLAINTEXT) (kafka.utils.ZkUtils$)
    [2015-07-21 06:53:17,058] INFO [Kafka Server 0], started (kafka.server.KafkaServer)

There are merely two commands to boot the entire environment up and that's it.

### Creating topic - `spark-topic`

**A topic** is where you're going to send messages to and where Spark Streaming is consuming them from later on. It's the communication channel between producers and consumers and you've got to have one.

Create `spark-topic` topic. The name is arbitrary and pick whatever makes you happy, but use it consistently in the other places where the name's used.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic spark-topic
    Created topic "spark-topic".

You may want to check out the available topics.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --list --zookeeper localhost:2181
    spark-topic

You're now done with setting up Kafka for the demo.

### (optional) Sending to and receiving messages from Kafka

Apache Kafka comes with two shell scripts to send and receive messages from topics. They're `kafka-console-producer.sh` and `kafka-console-consumer.sh`, respectively. They both use the console (stdin) as the input and output.

Let's publish few messages to the `spark-topic` topic using `./bin/kafka-console-producer.sh`.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic spark-topic
    hello
    hi
    ^D

 You can keep the producer up in one terminal and use another terminal to consume the messages or just send a couple of messages and close the session. Kafka persists messages for a period of time.

Consuming messages is as simple as running `./bin/kafka-console-consumer.sh`. Mind the option `--zookeeper` to point to Zookeeper where Kafka stores its configuration and `--from-beginning` that tells Kafka to process all persisted messages.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic spark-topic --from-beginning
    hello
    hi
    ^DConsumed 2 messages

## Spark time!

Remember `build.sbt` above that sets the Scala/sbt project up with appropriate Scala version and Spark Streaming dependencies?

    val sparkVersion = "1.4.1"
    scalaVersion := "2.11.7"

    libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-streaming" % sparkVersion,
      "org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
    )

To learn a little about the integration between Spark Streaming and Apache Kafka you're going to use `sbt console` and type all the integration code line by line in the interactive console. You could have a simple Scala application, but I'm leaving it to you as an exercise.

You may want to read the scaladoc of [org.apache.spark.SparkConf](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf) and [org.apache.spark.streaming.StreamingContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext) to learn about their purpose in the sample.

    scala> import org.apache.spark.SparkConf
    import org.apache.spark.SparkConf

    scala> val conf = new SparkConf().setMaster("local[*]").setAppName("KafkaReceiver")
    conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@2f8bf5fc

    scala> import org.apache.spark.streaming.StreamingContext
    import org.apache.spark.streaming.StreamingContext

    scala> import org.apache.spark.streaming.Seconds
    import org.apache.spark.streaming.Seconds

    scala> val ssc = new StreamingContext(conf, Seconds(10))
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    15/07/21 09:08:39 INFO SparkContext: Running Spark version 1.4.1
    ...
    ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2ce5cc3

    scala> import org.apache.spark.streaming.kafka.KafkaUtils
    import org.apache.spark.streaming.kafka.KafkaUtils

    // Note the name of the topic in use - spark-topic
    scala> val kafkaStream = KafkaUtils.createStream(ssc, "localhost:2181","spark-streaming-consumer-group", Map("spark-topic" -> 5))
    kafkaStream: org.apache.spark.streaming.dstream.ReceiverInputDStream[(String, String)] = org.apache.spark.streaming.kafka.KafkaInputDStream@4ab601ac

    // The very complex BIG data analytics
    scala> kafkaStream.print()

    // Start the streaming context so Spark Streaming polls for messages
    scala> ssc.start
    15/07/21 09:11:31 INFO ReceiverTracker: ReceiverTracker started
    15/07/21 09:11:31 INFO ForEachDStream: metadataCleanupDelay = -1
    ...
    15/07/21 09:11:31 INFO StreamingContext: StreamingContext started

Spark Streaming is now connected to Apache Kafka and consumes messages every 10 seconds. Leave it running and switch to another terminal.

Open a terminal to run a Kafka producer. You may want to use `kafkacat` (highly recommended) or the producer that comes with Apache Kafka - `kafka-console-producer.sh`.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic spark-topic
    hey spark, how are you doing today?

Switch to the terminal with Spark Streaming running and see the message printed out.

    15/07/21 09:12:00 INFO DAGScheduler: ResultStage 1 (print at <console>:18) finished in 0.016 s
    15/07/21 09:12:00 INFO DAGScheduler: Job 1 finished: print at <console>:18, took 0.030530 s
    -------------------------------------------
    Time: 1437462720000 ms
    -------------------------------------------
    (null,hey spark, how are you doing today?)

    15/07/21 09:12:00 INFO JobScheduler: Finished job streaming job 1437462720000 ms.1 from job set of time 1437462720000 ms

Congratulations! You've earned the Spark Streaming and Apache Kafka integration badge! Close Spark Streaming's context using

    scala> ssc.stop

or simply press `Ctrl+C`. Shut down Apache Kafka and Zookeeper, too. Done.

## (bonus) Building Apache Spark from the sources

You could use the very latest version of Spark Streaming in which the latest and greatest development is going on and lives on [the master branch](https://github.com/apache/spark/commits/master).

Following the official documentation [Building with build/mvn](http://spark.apache.org/docs/latest/building-spark.html#building-with-buildmvn)[^1], execute the following two commands. Please note the versions in the build as it uses **Scala 2.11.7** and **Hadoop 2.7.1**.

    ➜  spark git:(master) ./dev/change-version-to-2.11.sh
    ➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Dscala-2.11 -DskipTests clean install
    ...
    [INFO] ------------------------------------------------------------------------
    [INFO] Reactor Summary:
    [INFO]
    [INFO] Spark Project Parent POM ........................... SUCCESS [  2.696 s]
    [INFO] Spark Project Launcher ............................. SUCCESS [  7.346 s]
    [INFO] Spark Project Networking ........................... SUCCESS [  6.577 s]
    [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  4.052 s]
    [INFO] Spark Project Unsafe ............................... SUCCESS [  3.591 s]
    [INFO] Spark Project Core ................................. SUCCESS [01:43 min]
    [INFO] Spark Project Bagel ................................ SUCCESS [  4.704 s]
    [INFO] Spark Project GraphX ............................... SUCCESS [ 10.014 s]
    [INFO] Spark Project Streaming ............................ SUCCESS [ 21.804 s]
    [INFO] Spark Project Catalyst ............................. SUCCESS [ 28.271 s]
    [INFO] Spark Project SQL .................................. SUCCESS [ 34.811 s]
    [INFO] Spark Project ML Library ........................... SUCCESS [ 45.990 s]
    [INFO] Spark Project Tools ................................ SUCCESS [  1.964 s]
    [INFO] Spark Project Hive ................................. SUCCESS [ 39.831 s]
    [INFO] Spark Project REPL ................................. SUCCESS [  3.219 s]
    [INFO] Spark Project YARN ................................. SUCCESS [  5.109 s]
    [INFO] Spark Project Assembly ............................. SUCCESS [01:10 min]
    [INFO] Spark Project External Twitter ..................... SUCCESS [  5.204 s]
    [INFO] Spark Project External Flume Sink .................. SUCCESS [  5.472 s]
    [INFO] Spark Project External Flume ....................... SUCCESS [  6.292 s]
    [INFO] Spark Project External Flume Assembly .............. SUCCESS [  2.345 s]
    [INFO] Spark Project External MQTT ........................ SUCCESS [  5.361 s]
    [INFO] Spark Project External ZeroMQ ...................... SUCCESS [  6.006 s]
    [INFO] Spark Project External Kafka ....................... SUCCESS [ 13.850 s]
    [INFO] Spark Project Examples ............................. SUCCESS [01:15 min]
    [INFO] Spark Project External Kafka Assembly .............. SUCCESS [ 27.458 s]
    [INFO] Spark Project YARN Shuffle Service ................. SUCCESS [  4.636 s]
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 09:06 min
    [INFO] Finished at: 2015-07-21T06:48:02+02:00
    [INFO] Final Memory: 76M/905M
    [INFO] ------------------------------------------------------------------------

The jars for the version are at your command in the Maven local repository. Switch the version of Spark Streaming to **1.5.0-SNAPSHOT** and start over. It is known to work.

## Summary

As it turns out setting up a working configuration of Apache Kafka and Spark Streaming is just few clicks away. There are a couple of places that need improvement, but what the article has showed could be a good starting point for other real-time big data analytics using **Apache Kafka** as **the central hub for real-time streams of data** that are then processed **using complex algorithms** in **Spark Streaming**.

Once the data's processed, Spark Streaming could be publishing results into yet another Kafka topic and/or store in Hadoop for later. It seems I've got a very powerful setup I'm not yet fully aware of where I should apply to.

Let me know what you think about the topic[^2] of the blog post in the [Comments](#disqus_thread) section below or contact me at jacek@japila.pl. Follow the author as [@jaceklaskowski](https://twitter.com/jaceklaskowski) on Twitter, too.

[^1]: *I really really wish the Apache Spark project hadn't migrated the build to Apache Maven from the top-notch interactive build tool - [sbt](http://www.scala-sbt.org/)*
[^2]: pun intended
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Publishing Events Using Custom Producer for Apache Kafka]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html"/>
    <updated>2015-07-19T23:04:21+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka</id>
    <content type="html"><![CDATA[I'm a [Scala](http://www.scala-lang.org/) proponent so when I found out that [the Apache Kafka team has decided to switch to using Java as the main language of the new client API](https://cwiki.apache.org/confluence/display/KAFKA/Client+Rewrite) it was beyond my imagination. [Akka](http://akka.io/docs/)'s fine with their Java/Scala APIs and so I can't believe [Apache Kafka](http://kafka.apache.org/) couldn't offer similar APIs, too. It's even more weird when one finds out that Apache Kafka itself is written in Scala. Why on earth did they decide to do the migration?!

In order to learn Kafka better, I developed a custom producer using the latest Kafka's Producer API in Scala. I built Kafka from the sources, and so I'm using the version **0.8.3-SNAPSHOT**. It was pretty surprising experience, esp. when I ran across  [java.util.concurrent.Future](http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Future.html) that seems so limited to what [scala.concurrent.Future](http://docs.scala-lang.org/overviews/core/futures.html) offers. No `map`, `flatMap` or such? So far I consider the switch to using Java for the Client API a big mistake.

Here comes the complete Kafka producer I've developed in Scala that's supposed to serve as a basis for my future development endeavours using the API in what's going to be in 0.8.3 release.

<!-- more -->

## Custom KafkaProducer in Scala

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">java.util.concurrent.Future</span>
</span><span class='line'>
</span><span class='line'><span class="k">import</span> <span class="nn">org.apache.kafka.clients.producer.RecordMetadata</span>
</span><span class='line'>
</span><span class='line'><span class="k">object</span> <span class="nc">KafkaProducer</span> <span class="k">extends</span> <span class="nc">App</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">topic</span> <span class="k">=</span> <span class="n">util</span><span class="o">.</span><span class="nc">Try</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="mi">0</span><span class="o">)).</span><span class="n">getOrElse</span><span class="o">(</span><span class="s">&quot;my-topic-test&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">println</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;Connecting to $topic&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">import</span> <span class="nn">org.apache.kafka.clients.producer.KafkaProducer</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">props</span> <span class="k">=</span> <span class="k">new</span> <span class="n">java</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Properties</span><span class="o">()</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;localhost:9092&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;client.id&quot;</span><span class="o">,</span> <span class="s">&quot;KafkaProducer&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;key.serializer&quot;</span><span class="o">,</span> <span class="s">&quot;org.apache.kafka.common.serialization.IntegerSerializer&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;value.serializer&quot;</span><span class="o">,</span> <span class="s">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">producer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">KafkaProducer</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">String</span><span class="o">](</span><span class="n">props</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">import</span> <span class="nn">org.apache.kafka.clients.producer.ProducerRecord</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">polish</span> <span class="k">=</span> <span class="n">java</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">format</span><span class="o">.</span><span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="n">ofPattern</span><span class="o">(</span><span class="s">&quot;dd.MM.yyyy H:mm:ss&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">now</span> <span class="k">=</span> <span class="n">java</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="nc">LocalDateTime</span><span class="o">.</span><span class="n">now</span><span class="o">().</span><span class="n">format</span><span class="o">(</span><span class="n">polish</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">record</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ProducerRecord</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">String</span><span class="o">](</span><span class="n">topic</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="n">s</span><span class="s">&quot;hello at $now&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">metaF</span><span class="k">:</span> <span class="kt">Future</span><span class="o">[</span><span class="kt">RecordMetadata</span><span class="o">]</span> <span class="k">=</span> <span class="n">producer</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">meta</span> <span class="k">=</span> <span class="n">metaF</span><span class="o">.</span><span class="n">get</span><span class="o">()</span> <span class="c1">// blocking!</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">msgLog</span> <span class="k">=</span>
</span><span class='line'>    <span class="n">s</span><span class="s">&quot;&quot;&quot;</span>
</span><span class='line'><span class="s">       |offset    = ${meta.offset()}</span>
</span><span class='line'><span class="s">       |partition = ${meta.partition()}</span>
</span><span class='line'><span class="s">       |topic     = ${meta.topic()}</span>
</span><span class='line'><span class="s">     &quot;&quot;&quot;</span><span class="o">.</span><span class="n">stripMargin</span>
</span><span class='line'>  <span class="n">println</span><span class="o">(</span><span class="n">msgLog</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">producer</span><span class="o">.</span><span class="n">close</span><span class="o">()</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>

## Building Kafka from the sources

In order to run the client you should build Kafka from the sources first and publish the jars to the local Maven repository. The reason to have the build is that the producer uses the very latest Kafka Producer API.

Building Kafka from the sources is as simple as executing `gradle -PscalaVersion=2.11.7 clean releaseTarGz` in the directory where you `git clone https://github.com/apache/kafka.git`d [the Kafka repo from GitHub](https://github.com/apache/kafka.git).

    ➜  kafka git:(trunk) gradle -PscalaVersion=2.11.7 clean releaseTarGz
    Building project 'core' with Scala version 2.11.7
    ...
    BUILD SUCCESSFUL

    Total time: 1 mins 23.233 secs

I was building the distro against **Scala 2.11.7**.

Once done, `core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz` is where you find the release package.

    ➜  kafka git:(trunk) ✗ ls -l core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz
    -rw-r--r--  1 jacek  staff  17006303 18 lip 13:19 core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz

Unpack it and `cd` to it.

    ➜  kafka git:(trunk) ✗ tar -zxf core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz
    ➜  kafka git:(trunk) ✗ cd kafka_2.11-0.8.3-SNAPSHOT

## Zookeeper up and running

Running Zookeeper is the very first step you should do (as that's how Kafka maintains high-availability). Use `./bin/zookeeper-server-start.sh config/zookeeper.properties`:

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/zookeeper-server-start.sh config/zookeeper.properties
    [2015-07-20 00:17:08,134] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
    [2015-07-20 00:17:08,136] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
    [2015-07-20 00:17:08,136] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
    [2015-07-20 00:17:08,136] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
    [2015-07-20 00:17:08,136] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
    [2015-07-20 00:17:08,153] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
    [2015-07-20 00:17:08,154] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
    [2015-07-20 00:17:08,165] INFO Server environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT (org.apache.zookeeper.server.ZooKeeperServer)
    [2015-07-20 00:17:08,165] INFO Server environment:host.name=192.168.1.9 (org.apache.zookeeper.server.ZooKeeperServer)
    [2015-07-20 00:17:08,165] INFO Server environment:java.version=1.8.0_45 (org.apache.zookeeper.server.ZooKeeperServer)
    ...
    [2015-07-20 00:17:08,191] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)

## Kafka broker up and running

With Zookeeper up, start a Kafka broker using `./bin/kafka-server-start.sh config/server.properties` command:

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-server-start.sh config/server.properties
    [2015-07-20 00:18:33,574] INFO KafkaConfig values:
    	advertised.host.name = null
      ...
    	log.dir = /tmp/kafka-logs
      ...
    	zookeeper.connect = localhost:2181
    	zookeeper.sync.time.ms = 2000
    	port = 9092
      ...
    [2015-07-20 00:18:33,671] INFO starting (kafka.server.KafkaServer)
    [2015-07-20 00:18:33,673] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
    [2015-07-20 00:18:33,684] INFO Starting ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
    [2015-07-20 00:18:33,693] INFO Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT (org.apache.zookeeper.ZooKeeper)
    [2015-07-20 00:18:33,694] INFO Client environment:host.name=192.168.1.9 (org.apache.zookeeper.ZooKeeper)
    [2015-07-20 00:18:33,694] INFO Client environment:java.version=1.8.0_45 (org.apache.zookeeper.ZooKeeper)
    [2015-07-20 00:18:33,694] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
    ...
    [2015-07-20 00:18:34,414] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT -> EndPoint(192.168.1.9,9092,PLAINTEXT) (kafka.utils.ZkUtils$)
    [2015-07-20 00:18:34,419] INFO [Kafka Server 0], started (kafka.server.KafkaServer)

## Create topic

You're now going to create `my-topic` topic where the custom producer is going to publish messages to. Of course, the name of the topic is arbitrary, but should match what the custom producer uses.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic my-topic-test
    Created topic "my-topic-test".

Check out the topics available using `./bin/kafka-topics.sh --list --zookeeper localhost:2181`. You should see one.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --list --zookeeper localhost:2181
    my-topic-test

## Scala/sbt project setup

Create a Scala/sbt project with the following `build.sbt`:

    val kafkaVersion = "0.8.3-SNAPSHOT"

    scalaVersion := "2.11.7"

    resolvers += Resolver.mavenLocal

    libraryDependencies += "org.apache.kafka" % "kafka-clients" % kafkaVersion

Use the following `project/build.properties`:

    sbt.version=0.13.9-RC3

## Sending messages using KafkaProducer - `sbt run`

With the setup, you should now be able to run `sbt run` to run the custom Scala producer for Kafka.

    [kafka-publisher]> run
    [info] Running KafkaProducer
    Connecting to my-topic-test
    SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
    SLF4J: Defaulting to no-operation (NOP) logger implementation
    SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

    offset    = 0
    partition = 0
    topic     = my-topic-test

    [success] Total time: 8 s, completed Jul 20, 2015 12:29:44 AM

Executing `sbt run` again should show a different offset for the sam partition and topic:

    [kafka-publisher]> run
    [info] Running KafkaProducer
    Connecting to my-topic-test
    SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
    SLF4J: Defaulting to no-operation (NOP) logger implementation
    SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

    offset    = 1
    partition = 0
    topic     = my-topic-test

    [success] Total time: 0 s, completed Jul 20, 2015 12:30:47 AM

## Using kafkacat as a Kafka message consumer

If you really would like to see the message on the other, receiving side, I strongly recommend using [kafkacat](https://github.com/edenhill/kafkacat) that, once installed, boils down to the following command:

    ➜  ~  kafkacat -C -b localhost:9092 -t my-topic-test
    hello at 20.07.2015 0:29:43
    hello at 20.07.2015 0:30:46

It will read all the messages already published to `my-topic-test` topic and print out others once they come.

That's it. Congratulations!

## Summary

The complete project is [on GitHub in kafka-producer repo](https://github.com/jaceklaskowski/kafka-producer).

You may also want to read [1.3 Quick Start](http://kafka.apache.org/documentation.html#quickstart) in the official documentation of Apache Kafka.

Let me know what you think about the blog post in the [Comments](#disqus_thread) section below or contact me at jacek@japila.pl.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Kafka on Docker]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/07/14/apache-kafka-on-docker.html"/>
    <updated>2015-07-14T20:59:33+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/07/14/apache-kafka-on-docker</id>
    <content type="html"><![CDATA[[<img class="left" src="/images/kafka_logo.png" title="Apache Kafka" >](http://kafka.apache.org/)

[Apache Kafka](http://kafka.apache.org/) has always been high on my list of things to explore, but since there are quite a few things high on my list, Kafka couldn't actually make it to the very top. Until just recently, when I was asked to give **the broker** a try and see whether or not it meets a project's needs. Two projects, to be honest. You should see my face when I heard it.

[I compiled Apache Kafka from the sources](https://github.com/apache/kafka#apache-kafka), connected it to [Spark Streaming](https://spark.apache.org/streaming/) and even attempted to answer few questions on StackOverflow ([How to use Kafka in Flink using Scala?](http://stackoverflow.com/q/31391782/1305344) and [How to monitor Kafka broker using jmxtrans?](http://stackoverflow.com/q/31344222/1305344)), not to mention reading tons of articles and watching videos about the tool. I developed pretty strong confidence what use cases are the sweet spot for Apache Kafka.

With the team in [Codilime](http://www.codilime.com/) I'm developing [DeepSense.io](http://deepsense.io/) platform where we have just used [Ansible](http://www.ansible.com/home) to automate deployment. We've also been evaluating [Docker](https://www.docker.com/) and/or [Vagrant](https://www.vagrantup.com/). All to ease the deployment of DeepSense.io.

That's the moment when these two needs converged - exploring Apache Kafka and Docker (among the other tools) for three separate projects! Amazing, isn't it? I could finally explore how Docker might ease exploration of products and deployment. I knew Docker could ease my developer life, but it's only now when I really saw it. I would now *dockerize* everything. When I was told about the images [wurstmeister/kafka](https://registry.hub.docker.com/u/wurstmeister/kafka/) and [wurstmeister/zookeeper](https://registry.hub.docker.com/u/wurstmeister/zookeeper/) I couldn't have been happier. Running Apache Kafka and using Docker finally became a no-brainer and such a pleasant experience.

I then thought I'd share the love so it's not only mine and others could benefit from it, too.

<!-- more -->

Since I'm on **Mac OS X** the steps to run Apache Kafka using Docker rely on [boot2docker](http://boot2docker.io/) - *a Lightweight Linux for Docker* for platforms that don't natively support Docker - aforementioned Mac OS X and Windows.

You're going to use the images [wurstmeister/kafka](https://registry.hub.docker.com/u/wurstmeister/kafka/) and [wurstmeister/zookeeper](https://registry.hub.docker.com/u/wurstmeister/zookeeper/).

You can run containers off the images in background or foreground. Depending on you Unix skills, it means one or two terminals. Let's use two terminals for each server - Apache Kafka and Apache Zookeeper. I'm going to explain the role of Apache Zookeeper in another blog post.

Here come the steps to run Apache Kafka using Docker. It's assumed you've got `boot2docker` and `docker` tools installed.

    ➜  ~  boot2docker version
    Boot2Docker-cli version: v1.7.1
    Git commit: 8fdc6f5

    ➜  ~  docker --version
    Docker version 1.7.1, build 786b29d

I'm a big fan of [homebrew](http://brew.sh/) and highly recommend it to anyone using Mac OS X. Plenty of ready-to-use packages are just `brew install` away, docker and boot2docker including.

## Running Kafka on two Docker images

1. (Mac OS X and Windows users only) Execute `boot2docker up` to start the tiny Linux core on Mac OS.

        ➜  ~  boot2docker up
        Waiting for VM and Docker daemon to start...
        .o
        Started.
        Writing /Users/jacek/.boot2docker/certs/boot2docker-vm/ca.pem
        Writing /Users/jacek/.boot2docker/certs/boot2docker-vm/cert.pem
        Writing /Users/jacek/.boot2docker/certs/boot2docker-vm/key.pem

        To connect the Docker client to the Docker daemon, please set:
            export DOCKER_HOST=tcp://192.168.59.104:2376
            export DOCKER_CERT_PATH=/Users/jacek/.boot2docker/certs/boot2docker-vm
            export DOCKER_TLS_VERIFY=1

1. (Mac OS X and Windows users only) Execute `$(boot2docker shellinit)` to have the terminal set up and let `docker` know where the tiny Linux core is running (via `boot2docker`). You have to do the step in any terminal you open to work with Docker so the `export`s above are set. Should you face communication issues with `docker` commands, remember the step.

       ➜  ~  $(boot2docker shellinit)
       Writing /Users/jacek/.boot2docker/certs/boot2docker-vm/ca.pem
       Writing /Users/jacek/.boot2docker/certs/boot2docker-vm/cert.pem
       Writing /Users/jacek/.boot2docker/certs/boot2docker-vm/key.pem

1. Run `docker ps` to verify the terminal is configured properly for Docker.

       ➜  ~  docker ps
       CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

    No containers are running at this time. It's going to change soon once you start the containers for Zookeeper first and then Kafka.

1. [Create an account on Docker Hub](https://hub.docker.com/u/jaceklaskowski/) and execute `docker login` to store the credentials. With the step you don't have to repeat them for `docker pull` to pull images off the public hub of Docker images. Think of the Docker Hub as the GitHub for Docker images. Refer to the documentation [Using the Docker Hub](http://docs.docker.com/docker-hub/userguide/) for more up-to-date information.

1. Run `docker pull wurstmeister/zookeeper` to pull the Zookeeper image off Docker Hub (might take a few minutes to download):

       ➜  ~  docker pull wurstmeister/zookeeper
       Pulling repository wurstmeister/zookeeper
       a3075a3d32da: Download complete
       ...
       840840289a0d: Download complete
       e7381f1a45cf: Download complete
       5a6fc057f418: Download complete
       Status: Downloaded newer image for wurstmeister/zookeeper:latest

   You will see hashes of respective layers printed out to the console. It's expected.

1. Execute `docker pull wurstmeister/kafka` to pull the Kafka image off Docker Hub (might take a few minutes to download):

        ➜  ~  docker pull wurstmeister/kafka
        latest: Pulling from wurstmeister/kafka
        428b411c28f0: Pull complete
        ...
        422705fe88c8: Pull complete
        02bb7ca441d8: Pull complete
        0f9a08061516: Pull complete
        24fc32f98556: Already exists
        Digest: sha256:06150c136dcfe6e4fbbf37731a2119ea17a953c75902e52775b5511b3572aa1f
        Status: Downloaded newer image for wurstmeister/kafka:latest

1. Verify that the two images - `wurstmeister/kafka` and `wurstmeister/zookeeper` - are downloaded. From the command line execute `docker images`:

       ➜  ~  docker images
       REPOSITORY               TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
       wurstmeister/kafka       latest              24fc32f98556        3 weeks ago         477.6 MB
       wurstmeister/zookeeper   latest              a3075a3d32da        9 months ago        451 MB

1. You can now execute `docker run --name zookeeper -p 2181 -t wurstmeister/zookeeper` in one terminal to boot Zookeeper up.

    Remember `$(boot2docker shellinit)` if you're on Mac OS X or Windows.

        ➜  ~  docker run --name zookeeper -p 2181:2181 -t wurstmeister/zookeeper
        JMX enabled by default
        Using config: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
        2015-07-17 19:10:40,419 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
        ...
        2015-07-17 19:10:40,452 [myid:] - INFO  [main:ZooKeeperServer@773] - maxSessionTimeout set to -1
        2015-07-17 19:10:40,464 [myid:] - INFO  [main:NIOServerCnxnFactory@94] - binding to port 0.0.0.0/0.0.0.0:2181

    This gives you Zookeeper listening to port 2181. Check it out by telneting to it using docker (or boot2docker on MacOS) ip address.

        ➜  ~  telnet `boot2docker ip` 2181
        Trying 192.168.59.103...
        Connected to 192.168.59.103.
        Escape character is '^]'.

1. Execute `docker run --name kafka -e HOST_IP=localhost -e KAFKA_ADVERTISED_PORT=9092 -e KAFKA_BROKER_ID=1 -e ZK=zk -p 9092 --link zookeeper:zk -t wurstmeister/kafka` in another terminal.

    Remember `$(boot2docker shellinit)` if you're on Mac OS X or Windows.

        ➜  ~  docker run --name kafka -e HOST_IP=localhost -e KAFKA_ADVERTISED_PORT=9092 -e KAFKA_BROKER_ID=1 -e ZK=zk -p 9092 --link zookeeper:zk -t wurstmeister/kafka
        [2015-07-17 19:32:35,865] INFO Verifying properties (kafka.utils.VerifiableProperties)
        [2015-07-17 19:32:35,891] INFO Property advertised.port is overridden to 9092 (kafka.utils.VerifiableProperties)
        [2015-07-17 19:32:35,891] INFO Property broker.id is overridden to 1 (kafka.utils.VerifiableProperties)
        ...
        [2015-07-17 19:32:35,894] INFO Property zookeeper.connect is overridden to 172.17.0.5:2181 (kafka.utils.VerifiableProperties)
        [2015-07-17 19:32:35,895] INFO Property zookeeper.connection.timeout.ms is overridden to 6000 (kafka.utils.VerifiableProperties)
        [2015-07-17 19:32:35,924] INFO [Kafka Server 1], starting (kafka.server.KafkaServer)
        [2015-07-17 19:32:35,925] INFO [Kafka Server 1], Connecting to zookeeper on 172.17.0.5:2181 (kafka.server.KafkaServer)
        [2015-07-17 19:32:35,934] INFO Starting ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
        [2015-07-17 19:32:35,939] INFO Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT (org.apache.zookeeper.ZooKeeper)
        ...
        [2015-07-17 19:32:36,093] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
        [2015-07-17 19:32:36,095] INFO [Socket Server on Broker 1], Started (kafka.network.SocketServer)
        [2015-07-17 19:32:36,146] INFO Will not load MX4J, mx4j-tools.jar is not in the classpath (kafka.utils.Mx4jLoader$)
        [2015-07-17 19:32:36,172] INFO 1 successfully elected as leader (kafka.server.ZookeeperLeaderElector)
        [2015-07-17 19:32:36,253] INFO Registered broker 1 at path /brokers/ids/1 with address 61c359a3136b:9092. (kafka.utils.ZkUtils$)
        [2015-07-17 19:32:36,270] INFO [Kafka Server 1], started (kafka.server.KafkaServer)
        [2015-07-17 19:32:36,318] INFO New leader is 1 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener)

    You're now a happy user of Apache Kafka on your computer using Docker. Check the status of the containers using `docker ps`:

        ➜  ~  docker ps
        CONTAINER ID        IMAGE                    COMMAND                CREATED             STATUS              PORTS                                                 NAMES
        0b34a9927004        wurstmeister/kafka       "/bin/sh -c start-ka   2 minutes ago       Up 2 minutes        0.0.0.0:32769->9092/tcp                               kafka
        14fd32558b1c        wurstmeister/zookeeper   "/bin/sh -c '/usr/sb   4 minutes ago       Up 4 minutes        22/tcp, 2888/tcp, 3888/tcp, 0.0.0.0:32768->2181/tcp   zookeeper

1. Once you're done with your journey into Apache Kafka, `docker stop` the containers using `docker stop kafka zookeeper` (or `docker stop $(docker ps -aq)` if the only running containers are `kafka` and `zookeeper`).

        ➜  ~  docker stop kafka zookeeper
        kafka
        zookeeper

    Running `docker ps` shows no running containers afterwards:

        ➜  ~  docker ps
        CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

    There are no running containers since they're stopped now. They are still ready to be booted up again - use `docker ps -a` to see the ready-to-use containers:

        ➜  ~  docker ps -a
        CONTAINER ID        IMAGE                    COMMAND                CREATED             STATUS                        PORTS               NAMES
        7dde25ff7ec2        wurstmeister/kafka       "/bin/sh -c start-ka   15 hours ago        Exited (137) 16 seconds ago                       kafka
        b7b4b675b9c0        wurstmeister/zookeeper   "/bin/sh -c '/usr/sb   16 hours ago        Exited (137) 5 seconds ago                        zookeeper

1. (Mac OS X and Windows users only) Finally, stop `boot2docker` daemon using `boot2docker down`.

## Summary

With these two docker images - [wurstmeister/kafka](https://registry.hub.docker.com/u/wurstmeister/kafka/) and [wurstmeister/zookeeper](https://registry.hub.docker.com/u/wurstmeister/zookeeper/) - you can run **Apache Kafka** without much changing your local workstation to install it together with the necessary components like Apache ZooKeeper. You don't need to worry about upgrading the software and its dependencies except docker itself (and boot2docker if you're lucky to be on Mac OS). That saves you from spending time on installation and ensures proper functioning of your machine and the software. Moreover, the Docker images could be deployed to other machines and guarantee a consistent environment of the software inside.

Let me know what you think about the article in the [Comments](#disqus_thread) section below or contact me at jacek@japila.pl.
]]></content>
  </entry>
  
</feed>
