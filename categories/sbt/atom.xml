<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Sbt | Mastering FP and OO with Scala]]></title>
  <link href="http://blog.jaceklaskowski.pl/categories/sbt/atom.xml" rel="self"/>
  <link href="http://blog.jaceklaskowski.pl/"/>
  <updated>2015-07-21T21:20:47+02:00</updated>
  <id>http://blog.jaceklaskowski.pl/</id>
  <author>
    <name><![CDATA[Jacek Laskowski]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Real-time Data Processing Using Apache Kafka and Spark Streaming (and Scala and Sbt)]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/07/20/real-time-data-processing-using-apache-kafka-and-spark-streaming.html"/>
    <updated>2015-07-20T21:02:39+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/07/20/real-time-data-processing-using-apache-kafka-and-spark-streaming</id>
    <content type="html"><![CDATA[It's been a while since I worked with [Spark Streaming](http://spark.apache.org/streaming/). It was back then when I was working for a pet project that ultimately ended up as a Typesafe Activator template [Spark Streaming with Scala and Akka](http://www.typesafe.com/activator/template/spark-streaming-scala-akka) to get people going with the technologies.

Time flies by very quickly and as [the other blog posts](http://blog.jaceklaskowski.pl/2015/07/14/apache-kafka-on-docker.html) [may have showed](http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html) I'm evaluating [Apache Kafka](http://kafka.apache.org/) as a potential messaging and integration platform for my future projects. A lot is happening in so called *big data space* and Apache Kafka fits the bill in many dataflows around me so well. I'm very glad it's mostly all [Scala](http://www.scala-lang.org/) which we all *love* and are spending our time with. Ain't we?

From [Spark Streaming documentation](http://spark.apache.org/docs/latest/streaming-programming-guide.html) (Kafka bolded on purpose):

> Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like **Kafka**, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window.

Since Apache Kafka aims at being **the central hub for real-time streams of data** (see [1.2 Use Cases](http://kafka.apache.org/documentation.html#uses) and [Putting Apache Kafka To Use: A Practical Guide to Building a Stream Data Platform (Part 1)](http://www.confluent.io/blog/stream-data-platform-1/)) I couldn't deny myself the simple pleasure of giving it a go.

Buckle up and ingest *some* data using Apache Kafka and Spark Streaming! You surely *will* love the infrastructure (if you haven't already). Be sure to type fast to see the potential of the platform at your fingertips.

<!-- more -->

## The project (using sbt)

Here is the sbt build file `build.sbt` for the project:

    val sparkVersion = "1.4.1"
    scalaVersion := "2.11.7"

    libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-streaming" % sparkVersion,
      "org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
    )

It uses the latest released versions of **Spark Streaming 1.4.1** and **Scala 2.11.7**.

## Setting up Kafka broker

It assumes you've already installed Apache Kafka. You may want to use Docker (see [Apache Kafka on Docker](http://blog.jaceklaskowski.pl/2015/07/14/apache-kafka-on-docker.html)) or [build Kafka from the sources](http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html). Whatever approach you choose, start Zookeeper and Kafka.

### Starting Zookeeper

I'm using the version of Kafka built from the sources and `./bin/zookeeper-server-start.sh` that comes with it.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/zookeeper-server-start.sh config/zookeeper.properties
    ...
    [2015-07-21 06:51:39,614] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)

### Starting Kafka broker

Once Zookeeper is up and running (in the above case, listening to the port 2181), run a Kafka broker.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-server-start.sh config/server.properties
    ...
    [2015-07-21 06:53:17,051] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT -> EndPoint(192.168.1.9,9092,PLAINTEXT) (kafka.utils.ZkUtils$)
    [2015-07-21 06:53:17,058] INFO [Kafka Server 0], started (kafka.server.KafkaServer)

There are merely two commands to boot the entire environment up and that's it.

### Creating topic - `spark-topic`

**A topic** is where you're going to send messages to and where Spark Streaming is consuming them from later on. It's the communication channel between producers and consumers and you've got to have one.

Create `spark-topic` topic. The name is arbitrary and pick whatever makes you happy, but use it consistently in the other places where the name's used.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic spark-topic
    Created topic "spark-topic".

You may want to check out the available topics.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --list --zookeeper localhost:2181
    spark-topic

You're now done with setting up Kafka for the demo.

### (optional) Sending to and receiving messages from Kafka

Apache Kafka comes with two shell scripts to send and receive messages from topics. They're `kafka-console-producer.sh` and `kafka-console-consumer.sh`, respectively. They both use the console (stdin) as the input and output.

Let's publish few messages to the `spark-topic` topic using `./bin/kafka-console-producer.sh`.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic spark-topic
    hello
    hi
    ^D

 You can keep the producer up in one terminal and use another terminal to consume the messages or just send a couple of messages and close the session. Kafka persists messages for a period of time.

Consuming messages is as simple as running `./bin/kafka-console-consumer.sh`. Mind the option `--zookeeper` to point to Zookeeper where Kafka stores its configuration and `--from-beginning` that tells Kafka to process all persisted messages.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic spark-topic --from-beginning
    hello
    hi
    ^DConsumed 2 messages

## Spark time!

Remember `build.sbt` above that sets the Scala/sbt project up with appropriate Scala version and Spark Streaming dependencies?

    val sparkVersion = "1.4.1"
    scalaVersion := "2.11.7"

    libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-streaming" % sparkVersion,
      "org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
    )

To learn a little about the integration between Spark Streaming and Apache Kafka you're going to use `sbt console` and type all the integration code line by line in the interactive console. You could have a simple Scala application, but I'm leaving it to you as an exercise.

You may want to read the scaladoc of [org.apache.spark.SparkConf](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf) and [org.apache.spark.streaming.StreamingContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext) to learn about their purpose in the sample.

    scala> import org.apache.spark.SparkConf
    import org.apache.spark.SparkConf

    scala> val conf = new SparkConf().setMaster("local[*]").setAppName("KafkaReceiver")
    conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@2f8bf5fc

    scala> import org.apache.spark.streaming.StreamingContext
    import org.apache.spark.streaming.StreamingContext

    scala> import org.apache.spark.streaming.Seconds
    import org.apache.spark.streaming.Seconds

    scala> val ssc = new StreamingContext(conf, Seconds(10))
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    15/07/21 09:08:39 INFO SparkContext: Running Spark version 1.4.1
    ...
    ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2ce5cc3

    scala> import org.apache.spark.streaming.kafka.KafkaUtils
    import org.apache.spark.streaming.kafka.KafkaUtils

    // Note the name of the topic in use - spark-topic
    scala> val kafkaStream = KafkaUtils.createStream(ssc, "localhost:2181","spark-streaming-consumer-group", Map("spark-topic" -> 5))
    kafkaStream: org.apache.spark.streaming.dstream.ReceiverInputDStream[(String, String)] = org.apache.spark.streaming.kafka.KafkaInputDStream@4ab601ac

    // The very complex BIG data analytics
    scala> kafkaStream.print()

    // Start the streaming context so Spark Streaming polls for messages
    scala> ssc.start
    15/07/21 09:11:31 INFO ReceiverTracker: ReceiverTracker started
    15/07/21 09:11:31 INFO ForEachDStream: metadataCleanupDelay = -1
    ...
    15/07/21 09:11:31 INFO StreamingContext: StreamingContext started

Spark Streaming is now connected to Apache Kafka and consumes messages every 10 seconds. Leave it running and switch to another terminal.

Open a terminal to run a Kafka producer. You may want to use `kafkacat` (highly recommended) or the producer that comes with Apache Kafka - `kafka-console-producer.sh`.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic spark-topic
    hey spark, how are you doing today?

Switch to the terminal with Spark Streaming running and see the message printed out.

    15/07/21 09:12:00 INFO DAGScheduler: ResultStage 1 (print at <console>:18) finished in 0.016 s
    15/07/21 09:12:00 INFO DAGScheduler: Job 1 finished: print at <console>:18, took 0.030530 s
    -------------------------------------------
    Time: 1437462720000 ms
    -------------------------------------------
    (null,hey spark, how are you doing today?)

    15/07/21 09:12:00 INFO JobScheduler: Finished job streaming job 1437462720000 ms.1 from job set of time 1437462720000 ms

Congratulations! You've earned the Spark Streaming and Apache Kafka integration badge! Close Spark Streaming's context using

    scala> ssc.stop

or simply press `Ctrl+C`. Shut down Apache Kafka and Zookeeper, too. Done.

## (bonus) Building Apache Spark from the sources

You could use the very latest version of Spark Streaming in which the latest and greatest development is going on and lives on [the master branch](https://github.com/apache/spark/commits/master).

Following the official documentation [Building with build/mvn](http://spark.apache.org/docs/latest/building-spark.html#building-with-buildmvn)[^1], execute the following two commands. Please note the versions in the build as it uses **Scala 2.11.7** and **Hadoop 2.7.1**.

    ➜  spark git:(master) ./dev/change-version-to-2.11.sh
    ➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Dscala-2.11 -DskipTests clean install
    ...
    [INFO] ------------------------------------------------------------------------
    [INFO] Reactor Summary:
    [INFO]
    [INFO] Spark Project Parent POM ........................... SUCCESS [  2.696 s]
    [INFO] Spark Project Launcher ............................. SUCCESS [  7.346 s]
    [INFO] Spark Project Networking ........................... SUCCESS [  6.577 s]
    [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  4.052 s]
    [INFO] Spark Project Unsafe ............................... SUCCESS [  3.591 s]
    [INFO] Spark Project Core ................................. SUCCESS [01:43 min]
    [INFO] Spark Project Bagel ................................ SUCCESS [  4.704 s]
    [INFO] Spark Project GraphX ............................... SUCCESS [ 10.014 s]
    [INFO] Spark Project Streaming ............................ SUCCESS [ 21.804 s]
    [INFO] Spark Project Catalyst ............................. SUCCESS [ 28.271 s]
    [INFO] Spark Project SQL .................................. SUCCESS [ 34.811 s]
    [INFO] Spark Project ML Library ........................... SUCCESS [ 45.990 s]
    [INFO] Spark Project Tools ................................ SUCCESS [  1.964 s]
    [INFO] Spark Project Hive ................................. SUCCESS [ 39.831 s]
    [INFO] Spark Project REPL ................................. SUCCESS [  3.219 s]
    [INFO] Spark Project YARN ................................. SUCCESS [  5.109 s]
    [INFO] Spark Project Assembly ............................. SUCCESS [01:10 min]
    [INFO] Spark Project External Twitter ..................... SUCCESS [  5.204 s]
    [INFO] Spark Project External Flume Sink .................. SUCCESS [  5.472 s]
    [INFO] Spark Project External Flume ....................... SUCCESS [  6.292 s]
    [INFO] Spark Project External Flume Assembly .............. SUCCESS [  2.345 s]
    [INFO] Spark Project External MQTT ........................ SUCCESS [  5.361 s]
    [INFO] Spark Project External ZeroMQ ...................... SUCCESS [  6.006 s]
    [INFO] Spark Project External Kafka ....................... SUCCESS [ 13.850 s]
    [INFO] Spark Project Examples ............................. SUCCESS [01:15 min]
    [INFO] Spark Project External Kafka Assembly .............. SUCCESS [ 27.458 s]
    [INFO] Spark Project YARN Shuffle Service ................. SUCCESS [  4.636 s]
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 09:06 min
    [INFO] Finished at: 2015-07-21T06:48:02+02:00
    [INFO] Final Memory: 76M/905M
    [INFO] ------------------------------------------------------------------------

The jars for the version are at your command in the Maven local repository. Switch the version of Spark Streaming to **1.5.0-SNAPSHOT** and start over. It is known to work.

## Summary

As it turns out setting up a working configuration of Apache Kafka and Spark Streaming is just few clicks away. There are a couple of places that need improvement, but what the article has showed could be a good starting point for other real-time big data analytics using **Apache Kafka** as **the central hub for real-time streams of data** that are then processed **using complex algorithms** in **Spark Streaming**.

Once the data's processed, Spark Streaming could be publishing results into yet another Kafka topic and/or store in Hadoop for later. It seems I've got a very powerful setup I'm not yet fully aware of where I should apply to.

Let me know what you think about the topic[^2] of the blog post in the [Comments](#disqus_thread) section below or contact me at jacek@japila.pl. Follow the author as [@jaceklaskowski](https://twitter.com/jaceklaskowski) on Twitter, too.

[^1]: *I really really wish the Apache Spark project hadn't migrated the build to Apache Maven from the top-notch interactive build tool - [sbt](http://www.scala-sbt.org/)*
[^2]: pun intended
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using AutoPlugin in Sbt for Common Settings Across Projects in Multi-project Build]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/04/12/using-autoplugin-in-sbt-for-common-settings-across-projects-in-multi-project-build.html"/>
    <updated>2015-04-12T15:20:45+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/04/12/using-autoplugin-in-sbt-for-common-settings-across-projects-in-multi-project-build</id>
    <content type="html"><![CDATA[What a joy to learn all the goodies sbt brings to the table and be given a chance to apply it right away to commercial projects in Scala!

I've recently been assigned to a task to create a solution to share common settings across projects in a [multi-project build](http://www.scala-sbt.org/0.13/tutorial/Multi-Project.html) in a Scala project managed by [sbt](http://www.scala-sbt.org). With the new feature of sbt - [autoplugins](http://www.scala-sbt.org/0.13/docs/Plugins.html) - it was very easy to implement from the day one.

<!-- more -->

## The solution = project/CommonSettingsPlugin.scala

	import sbt._
	import Keys._

	object CommonSettingsPlugin extends AutoPlugin {
	  override def trigger = allRequirements
	  override lazy val projectSettings = Seq(
	    organization  := "io.deepsense",
	    version       := "0.1.0",
	    scalaVersion  := "2.11.6",
	    scalacOptions := Seq(
	      "-unchecked", "-deprecation", "-encoding", "utf8", "-feature",
	      "-language:existentials", "-language:implicitConversions"
	    )
	  )
	}

Since the autoplugin is automatically triggered for all the projects in the multi-project build -- `override def trigger = allRequirements` -- all the projects have `organization`, `version`, `scalaVersion`, `scalacOptions` set. No other work is needed.

Simple and easy, and, what's tracked under another issue, Jenkins should now be able to execute `coverageReport` and `coverageAggregate` without troubles for the project!

As a bonus, you don't have to change anything in your sbt project to leverage the simplicity - just drop the code as `project/CommonSettingsPlugin.scala` and do `show scalaVersion`. From that moment on, all the (sub)projects in your multi-project build should have the same `scalaVersion` (unless you use a separate `build.sbt` for a project and set `scalaVersion` explicitly).

## Lessons learnt

The initial mistake was to create a separate sbt project just to keep the autoplugin's code.

From the very first day of the plugin's life it was so much different from the other projects in the build -- it almost yelled out in pain at me not to place it where it was initially. It was a sbt plugin and as such it only meant to enhance sbt itself (not be a part of the commercial project).

What's even worse, up to the latest version of sbt 0.13.8 all plugins (are doomed to) use Scala 2.10.4 that's an issue for [sbt-scoverage plugin](https://github.com/scoverage/sbt-scoverage) as it kept refusing to work with the version of Scala.

All in all, I seemed reluctant for a long time to have even thought of a better place for the plugin even though I had actually known it.

A few discussions on [gitter on the sbt channel](https://gitter.im/sbt/sbt) made my day after the very helpful and nice people from the channel lent me a helping hand to find the final solution - the autoplugin went under `project` and...my live was so much bright again!

What was later pointed out to me when we discussed the change in the team, the final solution was a mere 24 line removal (!)

<img src="/images/using-autoplugin-patchset-gerrit.png" title="Patchset with AutoPlugin changes" >

It got `+2` from a teammate and has been merged to `master` without much hussle.

p.s. Shhh, the team thinks it was just me to have figured it out myself.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sbt-dependency-graph for Easier Dependency Management in Sbt]]></title>
    <link href="http://blog.jaceklaskowski.pl/2014/11/29/sbt-dependency-graph-for-easier-dependency-management-in-sbt.html"/>
    <updated>2014-11-29T23:10:06+01:00</updated>
    <id>http://blog.jaceklaskowski.pl/2014/11/29/sbt-dependency-graph-for-easier-dependency-management-in-sbt</id>
    <content type="html"><![CDATA[That's gonna be short and hopefully simple. If you're with [sbt](http://www.scala-sbt.org/) you're going to like [sbt-dependency-graph](https://github.com/jrudolph/sbt-dependency-graph) *plugin to create a dependency graph for your project* very much.

<!-- more -->

Edit `~/.sbt/0.13/plugins/sbt-dependency-graph.sbt` so it looks as follows:

	addSbtPlugin("net.virtual-void" % "sbt-dependency-graph" % "0.7.4")

Edit `~/.sbt/0.13/global.sbt` so it looks:

	net.virtualvoid.sbt.graph.Plugin.graphSettings

With these two files, open `sbt` or `activator` and execute `dependencyGraph` (I used [hello-slick-specs2](https://github.com/jaceklaskowski/hello-slick-specs2) project):

	> dependencyGraph
	[info] Updating {file:/Users/jacek/dev/oss/hello-slick-specs2/}hello-slick-specs2...
	[info] Resolving jline#jline;2.12 ...
	[info] Done updating.
	[info]                             +---------------------------+
	[info]                             |hello-slick-specs2_2.11 [S]|
	[info]                             |          default          |
	[info]                             |            1.0            |
	[info]                             +---------------------------+
	[info]                                    |     |   |    |
	[info]                ---------------------     |   |    ----------------------------
	[info]                |                         |   -----------------               |
	[info]                v                         v                   |               |
	[info]           +---------+          +------------------+          |               |
	[info]           |slf4j-nop|          |  slick_2.11 [S]  |          |               |
	[info]           |org.slf4j|          |com.typesafe.slick|          |               |
	[info]           |  1.7.7  |          |      2.1.0       |          |               |
	[info]           +---------+          +------------------+          |               |
	[info]                |                   |   ||      |             |               |
	[info]      -----------                   |   ||      ---------     |               |
	[info]      |  ----------------------------   ||              |     |               |
	[info]      |  |             ------------------|              |     |               |
	[info]      |  |             |                 |              |     |               |
	[info]      v  v             v                 v              v     v               v
	[info]  +---------+ +-----------------+ +------------+ +------------------+ +--------------+
	[info]  |slf4j-api| |    slf4j-api    | |   config   | |  scala-library   | |      h2      |
	[info]  |org.slf4j| |    org.slf4j    | |com.typesafe| |  org.scala-lang  | |com.h2database|
	[info]  |  1.7.7  | |      1.6.4      | |   1.2.1    | |      2.11.1      | |   1.4.182    |
	[info]  +---------+ |evicted by: 1.7.7| +------------+ |evicted by: 2.11.4| +--------------+
	[info]              +-----------------+                +------------------+
	[info] Note: The old tree layout is still available by using `dependency-tree`
	[success] Total time: 0 s, completed Nov 29, 2014 11:19:30 PM

Neat, isn't it?

You may also want to execute `dependencyGraphMl`:

	> dependencyGraphMl
	[info] Wrote dependency graph to '/Users/jacek/dev/oss/hello-slick-specs2/target/dependencies-compile.graphml'
	[success] Total time: 0 s, completed Nov 29, 2014 11:21:46 PM

Install [yEd](http://www.yworks.com/en/products/yfiles/yed/) and open the graph:

	> eval "open target/dependencies-compile.graphml" !
	[info] ans: Int = 0

<img src="/images/hello-slick-specs2-yed-graph.png" title="yEd graph of compile dependencies" >

I really wish I'd known it earlier. It'd surely have saved me a lot of time.]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Trait Init[Scope] in Sbt]]></title>
    <link href="http://blog.jaceklaskowski.pl/2014/07/22/trait-init-scope-in-sbt.html"/>
    <updated>2014-07-22T13:19:54+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2014/07/22/trait-init-scope-in-sbt</id>
    <content type="html"><![CDATA[It's been my wish to master [Scala](http://scala-lang.org/) recently and since I've been spending more time with [sbt](http://www.scala-sbt.org/) I've made the decision to use one to master the other (in no particular order). There are quite a few sophisticated projects in Scala out there, but sbt is enough for my needs.

In order to pursue my understanding of sbt (and hence Scala itself) I've been reading the sources that honestly keep surprising me so much often. It's almost every minute when I find myself scratching my head to digest a piece of sbt code. It's akin to when I was reading the source code of [Clojure](http://clojure.org/) to learn the language. People can write complicated code and I wouldn't be surprised to hear sbt's sources belong to the category. I don't care, though. I'm fine with the complexity hoping the mental pain brings me closer to master Scala.

Today I picked the trait [sbt.Init](https://github.com/sbt/sbt/blob/0.13/util/collection/src/main/scala/sbt/Settings.scala#L41) believing it'd be an important step in my journey.

**NOTE** It becomes feature-complete when the note disappears. Live with the few mistakes for now. Let me know what you think in the Comments section. The site is on GitHub so pull requests are warmly welcome, too. Thanks!

<!-- more -->

There’s the trait [sbt.Init](https://github.com/sbt/sbt/blob/0.13/util/collection/src/main/scala/sbt/Settings.scala#L41). I don’t really know what its purpose is and I hope to find it out after few Scala snippets. There’s just enough hope to master Scala while pursuing my understanding of sbt with the trait.

## Goal

Create an instance of trait `Init[Scope]`.

## Solution

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val init = new Init[Int] {
</span><span class='line'>  def showFullKey: Show[ScopedKey[_]] = Show { (sk: ScopedKey[_]) =&gt; 
</span><span class='line'>    s"${sk.scope}:${sk.key}...${sk.scopedKey}"
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>
Run `sbt` and then execute the command `consoleProject` to open sbt's Scala REPL with all the necessary types of sbt loaded.

## Mental issues encountered

1. I’m far from being able to distinguish easily type parameters, e.g. `Scope`, in parameterised types, e.g. `Init[Scope]`, from types themselves. When I see `Init[Scope]` my Java-trained eyes see `Scope` type within `Init` type and although it doesn’t make sense after a moment that’s my initial thought.

2. The type constructor `Show[ScopedKey[_]]` in the return value type of `showFullKey` is another trait `Show` that comes with `apply` that is supposed to return a `String` instance from `ScopedKey[_]`. But hey, `ScopedKey[_]` is another type constructor, and things get more complex for me again. Happily, `Show` has a companion object with `apply` method. The story ends as `ScopedKey` is a final parameterized case class and the function parameter `f: T => String` in `Show` returns a `String` so I've just merely followed the types and it *happened* to work fine. The Scala compiler happy and so am I.

## Summary

`Show` is a function type (with `apply`) that accepts `T` and returns `String`. In our case, `T` is `ScopedKey[_]` that’s...well...it’s yet to be understood.

## consoleProject in sbt

If you happened to want to see the code in action, execute `sbt consoleProject` and give the following a try:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// (attribute) key that points at Int value
</span><span class='line'>scala&gt; val number = AttributeKey[Int]("number", "number stringified")
</span><span class='line'>number: sbt.AttributeKey[Int] = number
</span><span class='line'> 
</span><span class='line'>scala&gt; val init = new Init[Int] {
</span><span class='line'>     |   def showFullKey: Show[ScopedKey[_]] = Show { (sk: ScopedKey[_]) =&gt;
</span><span class='line'>     |     s"${sk.scope}:${sk.key}...${sk.scopedKey}"
</span><span class='line'>     |   }
</span><span class='line'>     | }
</span><span class='line'>init: sbt.Init[Int] = $anon$1@1f95802
</span><span class='line'> 
</span><span class='line'>scala&gt; val sfk: Show[init.ScopedKey[_]] = init.showFullKey
</span><span class='line'>sfk: sbt.Show[init.ScopedKey[_]] = sbt.Show$$anon$1@7f54be72
</span><span class='line'>
</span><span class='line'>scala&gt; val s = sfk(init.ScopedKey[Int](scope=5, key=number))
</span><span class='line'>s: String = 5:number...ScopedKey(5,number)</span></code></pre></td></tr></table></div></figure>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Beginner "Guide" to Sbt 0.13 and IntelliJ IDEA 13]]></title>
    <link href="http://blog.jaceklaskowski.pl/2013/12/07/beginner-guide-to-sbt-0-dot-13-and-intellij-idea-13.html"/>
    <updated>2013-12-07T16:39:37+01:00</updated>
    <id>http://blog.jaceklaskowski.pl/2013/12/07/beginner-guide-to-sbt-0-dot-13-and-intellij-idea-13</id>
    <content type="html"><![CDATA[<img class="left" src="/images/intellij-idea-13-new-logo.png" title="The logo of IntelliJ IDEA 13" > It has not been very long ago when the only way to work with [sbt](http://www.scala-sbt.org/) projects in [IntelliJ IDEA](http://www.jetbrains.com/idea/) was to use the [sbt-idea](https://github.com/mpeltonen/sbt-idea) plugin that aimed at *"creating IntelliJ IDEA project files"*.

[With the recent release of IntelliJ IDEA 13](http://www.jetbrains.com/idea/whatsnew/) (build: 133.193,  released: December 3, 2013) it's no longer true - the version comes with built-in sbt support and the support is available in [the free Community Edition](http://www.jetbrains.com/idea/download/index.html), too.

My recent, rather quite frequent visits on [StackOverflow](http://stackoverflow.com/) have showed that there's one question very often asked - **How to start using sbt with IntelliJ IDEA?** It turns out that the latest version of IntelliJ IDEA 13 squashed it pretty neatly and the built-in sbt support made the question irrelevant.

Unless it changes, the page remains empty[^1]. What a surprise, isn't it? I've always been thinking about writing the best beginner guide and here it is...at long last!

On to querying [StackOverflow's #sbt tag](http://stackoverflow.com/questions/tagged/sbt) for questions about *a* sbt support in IntelliJ IDEA 13.

[^1]: as a kind of placeholder for future tips and tricks]]></content>
  </entry>
  
</feed>
