<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scala | Mastering FP and OO with Scala]]></title>
  <link href="http://blog.jaceklaskowski.pl/categories/scala/atom.xml" rel="self"/>
  <link href="http://blog.jaceklaskowski.pl/"/>
  <updated>2015-08-17T11:54:20+02:00</updated>
  <id>http://blog.jaceklaskowski.pl/</id>
  <author>
    <name><![CDATA[Jacek Laskowski]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Docker Your Scala Web Application (Play Framework)]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/07/24/docker-your-scala-web-application-play-framework.html"/>
    <updated>2015-07-24T23:03:07+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/07/24/docker-your-scala-web-application-play-framework</id>
    <content type="html"><![CDATA[We're experimenting with [Docker](https://www.docker.com/) in the [DeepSense.io](http://deepsense.io/) project. There might be a case or two in the other Scala company in Warsaw - [HCore](http://www.hcore.com/). I've also been noticing interest in using Docker in Scala projects in [Javeo](http://www.javeo.eu/) where the [Warsaw Scala Enthusiasts](http://www.meetup.com/WarszawScaLa/) meetups are taking place. The Docker space seems very hot for Scala developers in Warsaw, Poland. And these companies *are* hiring Scala developers!

I didn't know deploying Scala web applications might be so easy until the very recent Warsaw Scala Enthusiasts meetup when [Rafal Krzewski](http://www.meetup.com/WarszawScaLa/members/95521122/) introduced me to one of the two [sbt](http://www.scala-sbt.org/) plugins for Docker -  [sbt-native-packager](http://www.scala-sbt.org/sbt-native-packager/) (the other is [sbt-docker](https://github.com/marcuslonnberg/sbt-docker) that they say is even better).

The blog post shows how easy it is to use Docker as a means of deploying Scala web applications using Play Framework (that actually uses sbt-native-packager under the covers).

<!-- more -->

## Creating Play Framework web application

Create a new web application using Typesafe Activator tool using `activator new` command:

    ➜  docker-playground  activator new play-dockerized play-scala

    Fetching the latest list of templates...

    OK, application "play-dockerized" is being created using the "play-scala" template.

    To run "play-dockerized" from the command line, "cd play-dockerized" then:
    /Users/jacek/dev/sandbox/docker-playground/play-dockerized/activator run

    To run the test for "play-dockerized" from the command line, "cd play-dockerized" then:
    /Users/jacek/dev/sandbox/docker-playground/play-dockerized/activator test

    To run the Activator UI for "play-dockerized" from the command line, "cd play-dockerized" then:
    /Users/jacek/dev/sandbox/docker-playground/play-dockerized/activator ui

`cd` the `play-dockerized` directory and execute `sbt run` to start the application:

    ➜  play-dockerized  sbt run
    [info] Loading global plugins from /Users/jacek/.sbt/0.13/plugins
    [info] Updating {file:/Users/jacek/.sbt/0.13/plugins/}global-plugins...
    [info] Resolving org.fusesource.jansi#jansi;1.4 ...
    [info] Done updating.
    [info] Loading project definition from /Users/jacek/dev/sandbox/docker-playground/play-dockerized/project
    [info] Updating {file:/Users/jacek/dev/sandbox/docker-playground/play-dockerized/project/}play-dockerized-build...
    [info] Resolving org.fusesource.jansi#jansi;1.4 ...
    [info] Done updating.
    [info] Set current project to play-dockerized (in build file:/Users/jacek/dev/sandbox/docker-playground/play-dockerized/)
    [info] Updating {file:/Users/jacek/dev/sandbox/docker-playground/play-dockerized/}root...
    [info] Resolving jline#jline;2.12.1 ...
    [info] Done updating.

    --- (Running the application, auto-reloading is enabled) ---

    [info] p.a.l.c.ActorSystemProvider - Starting application default Akka system: application
    [info] p.c.s.NettyServer - Listening for HTTP on /0:0:0:0:0:0:0:0:9000

    (Server started, use Ctrl+D to stop and go back to the console...)

You should now be able to access http://localhost:9000. It's a vanilla **Play Framework 2.4.2** web application.

<img class="left" src="/images/docker-play-youre-using-play-2-4-2.png" title="Play Framework's default welcome page" >

## Publishing Docker image - `docker:publishLocal`

Play comes with **sbt-native-packager** plugin so stop the previous command (using Ctrl+D) and execute `sbt docker:publishLocal`:

    ➜  play-dockerized  sbt docker:publishLocal
    ...
    [info] Digest: sha256:66638b21de4b16af589f54cbd3e2698919efd529583b732a593613f35e813f0b
    [info] Status: Downloaded newer image for java:latest
    [info]  ---> 49ebfec495e1
    [info] Step 1 : WORKDIR /opt/docker
    [info]  ---> Running in ac01dbacaf66
    [info]  ---> 271ea5c0bd1e
    [info] Removing intermediate container ac01dbacaf66
    [info] Step 2 : ADD opt /opt
    [info]  ---> 9c423c2d2f0c
    [info] Removing intermediate container 3087077a2680
    [info] Step 3 : RUN chown -R daemon:daemon .
    [info]  ---> Running in bd40460a5e7d
    [info]  ---> aeec9392fc83
    [info] Removing intermediate container bd40460a5e7d
    [info] Step 4 : USER daemon
    [info]  ---> Running in 461907ca0474
    [info]  ---> 4f0ad20b6a7f
    [info] Removing intermediate container 461907ca0474
    [info] Step 5 : ENTRYPOINT bin/play-dockerized
    [info]  ---> Running in 09aa91f09bc5
    [info]  ---> 7f2afe7c4918
    [info] Removing intermediate container 09aa91f09bc5
    [info] Step 6 : CMD
    [info]  ---> Running in 99c12a3604a3
    [info]  ---> 617942a5bc6f
    [info] Removing intermediate container 99c12a3604a3
    [info] Successfully built 617942a5bc6f
    [info] Built image play-dockerized:1.0-SNAPSHOT
    [success] Total time: 101 s, completed Jul 23, 2015 8:31:18 AM

That was the exact moment when I realized how clever the sbt-native-packager plugin is to leverage well-known `publishLocal` task to publish to Docker repository (that's merely scoped to `docker` to change the way it works).

You've just created a brand new Docker image `play-dockerized:1.0-SNAPSHOT`. Use `docker images` command to check it out:

    ➜  play-dockerized  docker images
    REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
    play-dockerized     1.0-SNAPSHOT        617942a5bc6f        2 minutes ago       892.7 MB

## Docker my time!

You can start a container off the `play-dockerized` image using `docker run` command:

    ➜  play-dockerized  docker run --name play-8080 -p 8080:9000 play-dockerized:1.0-SNAPSHOT
    [info] - play.api.libs.concurrent.ActorSystemProvider - Starting application default Akka system: application
    [info] - play.api.Play - Application started (Prod)
    [info] - play.core.server.NettyServer - Listening for HTTP on /0:0:0:0:0:0:0:0:9000

The other command line options for `docker run` are `-p` to expose port `9000` outside Docker's virtual network (that's locally available as `8080`) and `--name` to give the container a friendly name (instead of relying on a cryptic hash).

<img class="left" src="/images/docker-play-new-app-ready.png" title="Play Framework inside Docker" >

In another terminal execute `docker ps` to see the container running:

    ➜  play-dockerized  docker ps -a
    CONTAINER ID        IMAGE                          COMMAND                CREATED             STATUS              PORTS                    NAMES
    511ca96e64a4        play-dockerized:1.0-SNAPSHOT   "bin/play-dockerized   10 minutes ago      Up 5 seconds        0.0.0.0:8080->9000/tcp   play-8080

Stop the container with `docker stop play-8080`. The Play Framework web app is no longer accessible. To start it again, execute `docker start play-8080`.

## Summary

It's so easy to have a Docker image of a Play Framework/Scala web application that I hardly believe I could have lived without using it for so long. Once an application becomes a Docker image you can use the other commands to play with it, mainly to deploy the image to any environment to have a consistent and exact environment. Love it so much now. And you can deploy the image to [Docker Hub](https://hub.docker.com/) (similarly how you publish the sources of your excellent applications to GitHub).

Let me know what you think about the topic of the blog post in the [Comments](#disqus_thread) section below or contact me at jacek@japila.pl. Follow the author as [@jaceklaskowski](https://twitter.com/jaceklaskowski) on Twitter, too.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Real-time Data Processing Using Apache Kafka and Spark Streaming (and Scala and Sbt)]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/07/20/real-time-data-processing-using-apache-kafka-and-spark-streaming.html"/>
    <updated>2015-07-20T21:02:39+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/07/20/real-time-data-processing-using-apache-kafka-and-spark-streaming</id>
    <content type="html"><![CDATA[It's been a while since I worked with [Spark Streaming](http://spark.apache.org/streaming/). It was back then when I was working for a pet project that ultimately ended up as a Typesafe Activator template [Spark Streaming with Scala and Akka](http://www.typesafe.com/activator/template/spark-streaming-scala-akka) to get people going with the technologies.

Time flies by very quickly and as [the other blog posts](http://blog.jaceklaskowski.pl/2015/07/14/apache-kafka-on-docker.html) [may have showed](http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html) I'm evaluating [Apache Kafka](http://kafka.apache.org/) as a potential messaging and integration platform for my future projects. A lot is happening in so called *big data space* and Apache Kafka fits the bill in many dataflows around me so well. I'm very glad it's mostly all [Scala](http://www.scala-lang.org/) which we all *love* and are spending our time with. Ain't we?

From [Spark Streaming documentation](http://spark.apache.org/docs/latest/streaming-programming-guide.html) (Kafka bolded on purpose):

> Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like **Kafka**, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window.

Since Apache Kafka aims at being **the central hub for real-time streams of data** (see [1.2 Use Cases](http://kafka.apache.org/documentation.html#uses) and [Putting Apache Kafka To Use: A Practical Guide to Building a Stream Data Platform (Part 1)](http://www.confluent.io/blog/stream-data-platform-1/)) I couldn't deny myself the simple pleasure of giving it a go.

Buckle up and ingest *some* data using Apache Kafka and Spark Streaming! You surely *will* love the infrastructure (if you haven't already). Be sure to type fast to see the potential of the platform at your fingertips.

<!-- more -->

## The project (using sbt)

Here is the sbt build file `build.sbt` for the project:

    val sparkVersion = "1.4.1"
    scalaVersion := "2.11.7"

    libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-streaming" % sparkVersion,
      "org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
    )

It uses the latest released versions of **Spark Streaming 1.4.1** and **Scala 2.11.7**.

## Setting up Kafka broker

It assumes you've already installed Apache Kafka. You may want to use Docker (see [Apache Kafka on Docker](http://blog.jaceklaskowski.pl/2015/07/14/apache-kafka-on-docker.html)) or [build Kafka from the sources](http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html). Whatever approach you choose, start Zookeeper and Kafka.

### Starting Zookeeper

I'm using the version of Kafka built from the sources and `./bin/zookeeper-server-start.sh` that comes with it.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/zookeeper-server-start.sh config/zookeeper.properties
    ...
    [2015-07-21 06:51:39,614] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)

### Starting Kafka broker

Once Zookeeper is up and running (in the above case, listening to the port 2181), run a Kafka broker.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-server-start.sh config/server.properties
    ...
    [2015-07-21 06:53:17,051] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT -> EndPoint(192.168.1.9,9092,PLAINTEXT) (kafka.utils.ZkUtils$)
    [2015-07-21 06:53:17,058] INFO [Kafka Server 0], started (kafka.server.KafkaServer)

There are merely two commands to boot the entire environment up and that's it.

### Creating topic - `spark-topic`

**A topic** is where you're going to send messages to and where Spark Streaming is consuming them from later on. It's the communication channel between producers and consumers and you've got to have one.

Create `spark-topic` topic. The name is arbitrary and pick whatever makes you happy, but use it consistently in the other places where the name's used.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic spark-topic
    Created topic "spark-topic".

You may want to check out the available topics.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --list --zookeeper localhost:2181
    spark-topic

You're now done with setting up Kafka for the demo.

### (optional) Sending to and receiving messages from Kafka

Apache Kafka comes with two shell scripts to send and receive messages from topics. They're `kafka-console-producer.sh` and `kafka-console-consumer.sh`, respectively. They both use the console (stdin) as the input and output.

Let's publish few messages to the `spark-topic` topic using `./bin/kafka-console-producer.sh`.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic spark-topic
    hello
    hi
    ^D

 You can keep the producer up in one terminal and use another terminal to consume the messages or just send a couple of messages and close the session. Kafka persists messages for a period of time.

Consuming messages is as simple as running `./bin/kafka-console-consumer.sh`. Mind the option `--zookeeper` to point to Zookeeper where Kafka stores its configuration and `--from-beginning` that tells Kafka to process all persisted messages.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic spark-topic --from-beginning
    hello
    hi
    ^DConsumed 2 messages

## Spark time!

Remember `build.sbt` above that sets the Scala/sbt project up with appropriate Scala version and Spark Streaming dependencies?

    val sparkVersion = "1.4.1"
    scalaVersion := "2.11.7"

    libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-streaming" % sparkVersion,
      "org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
    )

To learn a little about the integration between Spark Streaming and Apache Kafka you're going to use `sbt console` and type all the integration code line by line in the interactive console. You could have a simple Scala application, but I'm leaving it to you as an exercise.

You may want to read the scaladoc of [org.apache.spark.SparkConf](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf) and [org.apache.spark.streaming.StreamingContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext) to learn about their purpose in the sample.

    scala> import org.apache.spark.SparkConf
    import org.apache.spark.SparkConf

    scala> val conf = new SparkConf().setMaster("local[*]").setAppName("KafkaReceiver")
    conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@2f8bf5fc

    scala> import org.apache.spark.streaming.StreamingContext
    import org.apache.spark.streaming.StreamingContext

    scala> import org.apache.spark.streaming.Seconds
    import org.apache.spark.streaming.Seconds

    scala> val ssc = new StreamingContext(conf, Seconds(10))
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    15/07/21 09:08:39 INFO SparkContext: Running Spark version 1.4.1
    ...
    ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2ce5cc3

    scala> import org.apache.spark.streaming.kafka.KafkaUtils
    import org.apache.spark.streaming.kafka.KafkaUtils

    // Note the name of the topic in use - spark-topic
    scala> val kafkaStream = KafkaUtils.createStream(ssc, "localhost:2181","spark-streaming-consumer-group", Map("spark-topic" -> 5))
    kafkaStream: org.apache.spark.streaming.dstream.ReceiverInputDStream[(String, String)] = org.apache.spark.streaming.kafka.KafkaInputDStream@4ab601ac

    // The very complex BIG data analytics
    scala> kafkaStream.print()

    // Start the streaming context so Spark Streaming polls for messages
    scala> ssc.start
    15/07/21 09:11:31 INFO ReceiverTracker: ReceiverTracker started
    15/07/21 09:11:31 INFO ForEachDStream: metadataCleanupDelay = -1
    ...
    15/07/21 09:11:31 INFO StreamingContext: StreamingContext started

Spark Streaming is now connected to Apache Kafka and consumes messages every 10 seconds. Leave it running and switch to another terminal.

Open a terminal to run a Kafka producer. You may want to use `kafkacat` (highly recommended) or the producer that comes with Apache Kafka - `kafka-console-producer.sh`.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic spark-topic
    hey spark, how are you doing today?

Switch to the terminal with Spark Streaming running and see the message printed out.

    15/07/21 09:12:00 INFO DAGScheduler: ResultStage 1 (print at <console>:18) finished in 0.016 s
    15/07/21 09:12:00 INFO DAGScheduler: Job 1 finished: print at <console>:18, took 0.030530 s
    -------------------------------------------
    Time: 1437462720000 ms
    -------------------------------------------
    (null,hey spark, how are you doing today?)

    15/07/21 09:12:00 INFO JobScheduler: Finished job streaming job 1437462720000 ms.1 from job set of time 1437462720000 ms

Congratulations! You've earned the Spark Streaming and Apache Kafka integration badge! Close Spark Streaming's context using

    scala> ssc.stop

or simply press `Ctrl+C`. Shut down Apache Kafka and Zookeeper, too. Done.

## (bonus) Building Apache Spark from the sources

You could use the very latest version of Spark Streaming in which the latest and greatest development is going on and lives on [the master branch](https://github.com/apache/spark/commits/master).

Following the official documentation [Building Spark](http://spark.apache.org/docs/latest/building-spark.html)[^1], execute the following two commands. Please note the versions in the build as it uses **Scala 2.11.7** and **Hadoop 2.7.1**.

    ➜  spark git:(master) ./dev/change-scala-version.sh 2.11
    ➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Dscala-2.11 -DskipTests clean install
    ...
    [INFO] ------------------------------------------------------------------------
    [INFO] Reactor Summary:
    [INFO]
    [INFO] Spark Project Parent POM ........................... SUCCESS [  3.220 s]
    [INFO] Spark Project Launcher ............................. SUCCESS [ 11.681 s]
    [INFO] Spark Project Networking ........................... SUCCESS [  9.907 s]
    [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  6.187 s]
    [INFO] Spark Project Unsafe ............................... SUCCESS [  5.758 s]
    [INFO] Spark Project Core ................................. SUCCESS [02:43 min]
    [INFO] Spark Project Bagel ................................ SUCCESS [  8.570 s]
    [INFO] Spark Project GraphX ............................... SUCCESS [ 19.496 s]
    [INFO] Spark Project Streaming ............................ SUCCESS [ 36.986 s]
    [INFO] Spark Project Catalyst ............................. SUCCESS [ 57.976 s]
    [INFO] Spark Project SQL .................................. SUCCESS [ 57.685 s]
    [INFO] Spark Project ML Library ........................... SUCCESS [01:10 min]
    [INFO] Spark Project Tools ................................ SUCCESS [  5.609 s]
    [INFO] Spark Project Hive ................................. SUCCESS [ 46.276 s]
    [INFO] Spark Project REPL ................................. SUCCESS [  6.747 s]
    [INFO] Spark Project YARN ................................. SUCCESS [ 12.052 s]
    [INFO] Spark Project Assembly ............................. SUCCESS [01:11 min]
    [INFO] Spark Project External Twitter ..................... SUCCESS [  7.838 s]
    [INFO] Spark Project External Flume Sink .................. SUCCESS [  6.701 s]
    [INFO] Spark Project External Flume ....................... SUCCESS [  9.614 s]
    [INFO] Spark Project External Flume Assembly .............. SUCCESS [  2.135 s]
    [INFO] Spark Project External MQTT ........................ SUCCESS [  7.247 s]
    [INFO] Spark Project External ZeroMQ ...................... SUCCESS [  7.550 s]
    [INFO] Spark Project External Kafka ....................... SUCCESS [ 12.960 s]
    [INFO] Spark Project Examples ............................. SUCCESS [01:28 min]
    [INFO] Spark Project External Kafka Assembly .............. SUCCESS [ 27.143 s]
    [INFO] Spark Project YARN Shuffle Service ................. SUCCESS [  7.143 s]
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 12:49 min
    [INFO] Finished at: 2015-07-22T11:02:46+02:00
    [INFO] Final Memory: 85M/958M
    [INFO] ------------------------------------------------------------------------

The jars for the version are at your command in the Maven local repository. Switch the version of Spark Streaming to **1.5.0-SNAPSHOT** and start over.

## Summary

As it turns out setting up a working configuration of Apache Kafka and Spark Streaming is just few clicks away. There are a couple of places that need improvement, but what the article has showed could be a good starting point for other real-time big data analytics using **Apache Kafka** as **the central hub for real-time streams of data** that are then processed **using complex algorithms** in **Spark Streaming**.

Once the data's processed, Spark Streaming could be publishing results into yet another Kafka topic and/or store in Hadoop for later. It seems I've got a very powerful setup I'm not yet fully aware of where I should apply to.

Let me know what you think about the topic[^2] of the blog post in the [Comments](#disqus_thread) section below or contact me at jacek@japila.pl. Follow the author as [@jaceklaskowski](https://twitter.com/jaceklaskowski) on Twitter, too.

[^1]: *I really really wish the Apache Spark project hadn't migrated the build to Apache Maven from the top-notch interactive build tool - [sbt](http://www.scala-sbt.org/)*
[^2]: pun intended
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Publishing Events Using Custom Producer for Apache Kafka]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html"/>
    <updated>2015-07-19T23:04:21+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka</id>
    <content type="html"><![CDATA[I'm a [Scala](http://www.scala-lang.org/) proponent so when I found out that [the Apache Kafka team has decided to switch to using Java as the main language of the new client API](https://cwiki.apache.org/confluence/display/KAFKA/Client+Rewrite) it was beyond my imagination. [Akka](http://akka.io/docs/)'s fine with their Java/Scala APIs and so I can't believe [Apache Kafka](http://kafka.apache.org/) couldn't offer similar APIs, too. It's even more weird when one finds out that Apache Kafka itself is written in Scala. Why on earth did they decide to do the migration?!

In order to learn Kafka better, I developed a custom producer using the latest Kafka's Producer API in Scala. I built Kafka from the sources, and so I'm using the version **0.8.3-SNAPSHOT**. It was pretty surprising experience, esp. when I ran across  [java.util.concurrent.Future](http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Future.html) that seems so limited to what [scala.concurrent.Future](http://docs.scala-lang.org/overviews/core/futures.html) offers. No `map`, `flatMap` or such? So far I consider the switch to using Java for the Client API a big mistake.

Here comes the complete Kafka producer I've developed in Scala that's supposed to serve as a basis for my future development endeavours using the API in what's going to be in 0.8.3 release.

<!-- more -->

## Custom KafkaProducer in Scala

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">java.util.concurrent.Future</span>
</span><span class='line'>
</span><span class='line'><span class="k">import</span> <span class="nn">org.apache.kafka.clients.producer.RecordMetadata</span>
</span><span class='line'>
</span><span class='line'><span class="k">object</span> <span class="nc">KafkaProducer</span> <span class="k">extends</span> <span class="nc">App</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">topic</span> <span class="k">=</span> <span class="n">util</span><span class="o">.</span><span class="nc">Try</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="mi">0</span><span class="o">)).</span><span class="n">getOrElse</span><span class="o">(</span><span class="s">&quot;my-topic-test&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">println</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;Connecting to $topic&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">import</span> <span class="nn">org.apache.kafka.clients.producer.KafkaProducer</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">props</span> <span class="k">=</span> <span class="k">new</span> <span class="n">java</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Properties</span><span class="o">()</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;localhost:9092&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;client.id&quot;</span><span class="o">,</span> <span class="s">&quot;KafkaProducer&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;key.serializer&quot;</span><span class="o">,</span> <span class="s">&quot;org.apache.kafka.common.serialization.IntegerSerializer&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;value.serializer&quot;</span><span class="o">,</span> <span class="s">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">producer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">KafkaProducer</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">String</span><span class="o">](</span><span class="n">props</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">import</span> <span class="nn">org.apache.kafka.clients.producer.ProducerRecord</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">polish</span> <span class="k">=</span> <span class="n">java</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">format</span><span class="o">.</span><span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="n">ofPattern</span><span class="o">(</span><span class="s">&quot;dd.MM.yyyy H:mm:ss&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">now</span> <span class="k">=</span> <span class="n">java</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="nc">LocalDateTime</span><span class="o">.</span><span class="n">now</span><span class="o">().</span><span class="n">format</span><span class="o">(</span><span class="n">polish</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">record</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ProducerRecord</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">String</span><span class="o">](</span><span class="n">topic</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="n">s</span><span class="s">&quot;hello at $now&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">metaF</span><span class="k">:</span> <span class="kt">Future</span><span class="o">[</span><span class="kt">RecordMetadata</span><span class="o">]</span> <span class="k">=</span> <span class="n">producer</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">meta</span> <span class="k">=</span> <span class="n">metaF</span><span class="o">.</span><span class="n">get</span><span class="o">()</span> <span class="c1">// blocking!</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">msgLog</span> <span class="k">=</span>
</span><span class='line'>    <span class="n">s</span><span class="s">&quot;&quot;&quot;</span>
</span><span class='line'><span class="s">       |offset    = ${meta.offset()}</span>
</span><span class='line'><span class="s">       |partition = ${meta.partition()}</span>
</span><span class='line'><span class="s">       |topic     = ${meta.topic()}</span>
</span><span class='line'><span class="s">     &quot;&quot;&quot;</span><span class="o">.</span><span class="n">stripMargin</span>
</span><span class='line'>  <span class="n">println</span><span class="o">(</span><span class="n">msgLog</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">producer</span><span class="o">.</span><span class="n">close</span><span class="o">()</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>

## Building Kafka from the sources

In order to run the client you should build Kafka from the sources first and publish the jars to the local Maven repository. The reason to have the build is that the producer uses the very latest Kafka Producer API.

Building Kafka from the sources is as simple as executing `gradle -PscalaVersion=2.11.7 clean releaseTarGz` in the directory where you `git clone https://github.com/apache/kafka.git`d [the Kafka repo from GitHub](https://github.com/apache/kafka.git).

    ➜  kafka git:(trunk) gradle -PscalaVersion=2.11.7 clean releaseTarGz
    Building project 'core' with Scala version 2.11.7
    ...
    BUILD SUCCESSFUL

    Total time: 1 mins 23.233 secs

I was building the distro against **Scala 2.11.7**.

Once done, `core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz` is where you find the release package.

    ➜  kafka git:(trunk) ✗ ls -l core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz
    -rw-r--r--  1 jacek  staff  17006303 18 lip 13:19 core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz

Unpack it and `cd` to it.

    ➜  kafka git:(trunk) ✗ tar -zxf core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz
    ➜  kafka git:(trunk) ✗ cd kafka_2.11-0.8.3-SNAPSHOT

## Zookeeper up and running

Running Zookeeper is the very first step you should do (as that's how Kafka maintains high-availability). Use `./bin/zookeeper-server-start.sh config/zookeeper.properties`:

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/zookeeper-server-start.sh config/zookeeper.properties
    [2015-07-20 00:17:08,134] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
    [2015-07-20 00:17:08,136] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
    [2015-07-20 00:17:08,136] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
    [2015-07-20 00:17:08,136] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
    [2015-07-20 00:17:08,136] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
    [2015-07-20 00:17:08,153] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
    [2015-07-20 00:17:08,154] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
    [2015-07-20 00:17:08,165] INFO Server environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT (org.apache.zookeeper.server.ZooKeeperServer)
    [2015-07-20 00:17:08,165] INFO Server environment:host.name=192.168.1.9 (org.apache.zookeeper.server.ZooKeeperServer)
    [2015-07-20 00:17:08,165] INFO Server environment:java.version=1.8.0_45 (org.apache.zookeeper.server.ZooKeeperServer)
    ...
    [2015-07-20 00:17:08,191] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)

## Kafka broker up and running

With Zookeeper up, start a Kafka broker using `./bin/kafka-server-start.sh config/server.properties` command:

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-server-start.sh config/server.properties
    [2015-07-20 00:18:33,574] INFO KafkaConfig values:
    	advertised.host.name = null
      ...
    	log.dir = /tmp/kafka-logs
      ...
    	zookeeper.connect = localhost:2181
    	zookeeper.sync.time.ms = 2000
    	port = 9092
      ...
    [2015-07-20 00:18:33,671] INFO starting (kafka.server.KafkaServer)
    [2015-07-20 00:18:33,673] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
    [2015-07-20 00:18:33,684] INFO Starting ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
    [2015-07-20 00:18:33,693] INFO Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT (org.apache.zookeeper.ZooKeeper)
    [2015-07-20 00:18:33,694] INFO Client environment:host.name=192.168.1.9 (org.apache.zookeeper.ZooKeeper)
    [2015-07-20 00:18:33,694] INFO Client environment:java.version=1.8.0_45 (org.apache.zookeeper.ZooKeeper)
    [2015-07-20 00:18:33,694] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
    ...
    [2015-07-20 00:18:34,414] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT -> EndPoint(192.168.1.9,9092,PLAINTEXT) (kafka.utils.ZkUtils$)
    [2015-07-20 00:18:34,419] INFO [Kafka Server 0], started (kafka.server.KafkaServer)

## Creating topic

You're now going to create `my-topic` topic where the custom producer is going to publish messages to. Of course, the name of the topic is arbitrary, but should match what the custom producer uses.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic my-topic-test
    Created topic "my-topic-test".

Check out the topics available using `./bin/kafka-topics.sh --list --zookeeper localhost:2181`. You should see one.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --list --zookeeper localhost:2181
    my-topic-test

## Scala/sbt project setup

Create a Scala/sbt project with the following `build.sbt`:

    val kafkaVersion = "0.8.3-SNAPSHOT"

    scalaVersion := "2.11.7"

    resolvers += Resolver.mavenLocal

    libraryDependencies += "org.apache.kafka" % "kafka-clients" % kafkaVersion

Use the following `project/build.properties`:

    sbt.version=0.13.9-RC3

## Sending messages using KafkaProducer - `sbt run`

With the setup, you should now be able to run `sbt run` to run the custom Scala producer for Kafka.

    [kafka-publisher]> run
    [info] Running KafkaProducer
    Connecting to my-topic-test
    SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
    SLF4J: Defaulting to no-operation (NOP) logger implementation
    SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

    offset    = 0
    partition = 0
    topic     = my-topic-test

    [success] Total time: 8 s, completed Jul 20, 2015 12:29:44 AM

Executing `sbt run` again should show a different offset for the sam partition and topic:

    [kafka-publisher]> run
    [info] Running KafkaProducer
    Connecting to my-topic-test
    SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
    SLF4J: Defaulting to no-operation (NOP) logger implementation
    SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

    offset    = 1
    partition = 0
    topic     = my-topic-test

    [success] Total time: 0 s, completed Jul 20, 2015 12:30:47 AM

## Using kafkacat as a Kafka message consumer

If you really would like to see the message on the other, receiving side, I strongly recommend using [kafkacat](https://github.com/edenhill/kafkacat) that, once installed, boils down to the following command:

    ➜  ~  kafkacat -C -b localhost:9092 -t my-topic-test
    hello at 20.07.2015 0:29:43
    hello at 20.07.2015 0:30:46

It will read all the messages already published to `my-topic-test` topic and print out others once they come.

That's it. Congratulations!

## Summary

The complete project is [on GitHub in kafka-producer repo](https://github.com/jaceklaskowski/kafka-producer).

You may also want to read [1.3 Quick Start](http://kafka.apache.org/documentation.html#quickstart) in the official documentation of Apache Kafka.

Let me know what you think about the topic[^1] of the blog post in the [Comments](#disqus_thread) section below or contact me at jacek@japila.pl. Follow the author as [@jaceklaskowski](https://twitter.com/jaceklaskowski) on Twitter, too.

[^1]: pun intended
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ad Hoc Polymorphism in Scala With Type Classes]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/05/15/ad-hoc-polymorphism-in-scala-with-type-classes.html"/>
    <updated>2015-05-15T22:21:20+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/05/15/ad-hoc-polymorphism-in-scala-with-type-classes</id>
    <content type="html"><![CDATA[My journey into the depths of [Scala](http://www.scala-lang.org/) is in full swing. Not only can I learn the theory (with the group of [Warsaw Scala Enthusiasts](http://warsawscala.pl)), but also apply it to commercial projects (with the Scala development teams of [DeepSense.io](http://deepsense.io/) and [HCore](http://www.hcore.com/)). Each day I feel I'm getting better at using **type system in Scala** in a more concious and (hopefully) efficient manner.

This time I sank into **type classes** that is a means of doing ** *ad hoc* polymorphism** in Scala.

From [*ad hoc* polymorphism](http://en.wikipedia.org/wiki/Ad_hoc_polymorphism) article on Wikipedia:

> In programming languages, **ad hoc polymorphism** is a kind of polymorphism in which polymorphic functions can be applied to arguments of different types, because a polymorphic function can denote a number of distinct and potentially heterogeneous implementations depending on the type of argument(s) to which it is applied.

The blog post presents a way to implement the type classes concept in Scala.

p.s. I'm yet to find out how much of it is [multimethods](http://clojure.org/multimethods) in [Clojure](http://clojure.org/) (that was once of much help to introduce me to **functional programming**).

<!-- more -->

## Theory

From the article [Polymorphism (computer science)](http://en.wikipedia.org/wiki/Polymorphism_(computer_science)) on Wikipedia, you can read about three different kinds of polymorphism:

* **subtyping**
* **parametric polymorphism**
* **ad hoc polymorphism**

The order does matter (and is different from the Wikipedia article's one) as I think that's exactly the order of when and how well developers master them (in any programming language).

I reckon the Scala community finds the first two quite often used in code that contributes to how well it's understood and applied (except [variance](http://en.wikipedia.org/wiki/Covariance_and_contravariance_%28computer_science%29)), and just the last one - ad hoc polymorphism - is proving a major hurdle for many, me including. It didn't click for me for a long time, either, only until I found the "venues" of very concise and comprehensible material (see [References](#references) section below).

> In programming languages and type theory, polymorphism is the provision of a single interface to entities of different types.

The difference between *subtyping* and *parametric polymorphism* and *ad hoc polymorphism* is that the type hierarchy is expressed explicitly using **extends** keyword in Scala (for subtyping) or type parameters (for parametric polymorphism) while ad hoc polymorphism bases itself on **implicit classes** to *mixin* the behaviour (using traits). And that's pretty much all, technically.

Let's see it in a Scala code.

## Practice

### Algebraic data types (raw data)

Let's assume you have the following type hierarchy (from [Tutorial: Typeclasses in Scala with Dan Rosen](https://youtu.be/sVMES4RZF-8)):

    sealed trait Expression
    case class Number(value: Int) extends Expression
    case class Plus(lhs: Expression, rhs: Expression) extends Expression
    case class Minus(lhs: Expression, rhs: Expression) extends Expression

No behaviour, but *algebraic data types* or (often called) *raw data*.

Think about how you'd go about evaluating expressions, i.e. `Plus(Minus(Number(5), Number(3)), Number(18))`, i.e. how to make the following application compile and print `20`?

    object Main extends App {
      val expr = Plus(Minus(Number(5), Number(3)), Number(18))
      println(...) // do something with expr so it prints 20
    }

The past me would change `sealed trait Expression` to include `def value: Int` and force the three case classes `Number`, `Plus` and `Minus` follow. I'd not be surprised if you thought exactly so. You should not, however.

Think about the case where you must **not** change them as they could be provided as a library or be part of the language or be licensed or...I simply ask you not to.

### objects to apply behaviour

Since you're in Scala, you could easily work it around with an object, say `object ExpressionEvaluator`, that would *pattern match* to types and do the heavy lifting, i.e. know what to do with each and every type:

    object ExpressionEvaluator {
      def value(expression: Expression): Int = expression match {
        case Number(value) => value
        case Plus(lhs, rhs) => value(lhs) + value(rhs)
        case Minus(lhs, rhs) => value(lhs) - value(rhs)
      }
    }

That's quite inefficient and cumbersome, though. You'd have to know about all of the implementations as well as about what to do for each. It's nearly impossible to have right, complete and still flexible.

With the above object, you'd write:

    object Main extends App {
      val expr = Plus(Minus(Number(5), Number(3)), Number(18))
      println(ExpressionEvaluator.value(expr)) // print the value
    }

### implicits in Scala

Let's do it the right way, so it's not `ExpressionEvaluator` to know what to do with every `Expression`, but expressions themselves do what they're supposed to do instead.

You may have heard about **implicits** in Scala. Perhaps, you may have used them, too. Think about a solution with implicit machinery so the following is possible:

    object Main extends App {
      val expr = Plus(Minus(Number(5), Number(3)), Number(18))
      println(expr.value) // print the value
    }

One way in Scala would be to apply **Pimp my Library** pattern leveraging `implicit class`es to add necessary methods as follows:

    implicit class ExpressionOps(e: Expression) {
      def value = ... // calculate the value
    }

And have a `implicit class` per `case class` and `sealed trait Expression`, too. The reason to have the implicits is to add a `value` method to every type, i.e. implicitly convert types without `value` to ones that have it.

    implicit class ExpressionOps(e: Expression) {
      def value: Int = e match {
        case n : Number => n.value
        case p : Plus => p.value
        case m : Minus => m.value
      }
    }

    implicit class PlusOps(p: Plus) {
      def value: Int = p.lhs.value + p.rhs.value
    }

    implicit class MinusOps(m: Minus) {
      def value: Int = m.lhs.value - m.rhs.value
    }

Note that since `Number` had `value` already, an implicit was not needed.

With the implicits in place, you can write:

    println(expr.value)  // prints 20

The implicit-based solution is far much more flexible because calculating a value is the responsibility of an implicit in scope -- a change in a single implicit, say `MinusOps`, would only change how `Minus.value` works (with no changes to the rest of the "framework").

You're halfway to typeclasses!

### Partial ad hoc polymorphism

Think about the case where you'd have a library to calculate a value of `Valueable` values (no pun intended). Say, you have such a library that offers the following "calculator":

    def calculate(v: Valueable) = ... // calculate the value of v

How would you go about converting `Expression`s to `Valueable`s so a value of `Expression` type would participate in the contract of `def calculate`? 

You're right that whenever type conversion is needed in Scala, implicits have their say -- you used them already to have `def value` for `Expression` type hierarchy. You're going to use them again with a very small change that has enormous impact. Doing so will introduce the type class design pattern.

The current solution relies on values with `def value` -- it's like having a library that uses structural typing in Scala that unfortunatelly uses reflection and hence is very costly performance-wise. Happily, you don't need structural types.

If you had a library that expects values of some type, say `trait Valueable`, to calculate a value or some other library to JSONify them (using some `trait Json`), the previous solutions would fall short -- you've merely met the requirement of being able to call a `value` method on values, but the values don't belong to a single type hierarchy of some `trait Valueable` that the library uses.

Assume you have a library that works with `trait Valueable`-only values and there's a `calculate` method to work with them as follows:

    trait Valueable {
      def value: Int
    }

    def calculate(v: Valueable) = v.value

Note that the library knows nothing about the `Expression` type hierarchy. The `Expression` type hierarchy could not even have existed at the time `Valueable` did.

A more flexible and efficient solution is to *somehow* meld the `Expression` type hierarchy with `Valueable` and create *is-a relationship*. No, no, you're not going to change the `Expression` trait in any way. You must **not** and you even **can't**, remember?

Welcome to **typeclasses** (also known as **type class design pattern**)!

In order to have `extends`-like semantic at runtime with no `extends Valueable` in the (closed and `sealed`) `Expression` type hierarchy you change `implicit class`es to have the common trait mixed in, instead.

    implicit class ExpressionOps(e: Expression) extends Valueable {
      def value = e match {
        case n: Number => n.value
        case p: Plus => p.value
        case m: Minus => m.value
      }
    }

    implicit class PlusOps(p: Plus) {
      def value: Int = p.lhs.value + p.rhs.value
    }

    implicit class MinusOps(m: Minus) {
      def value: Int = m.lhs.value - m.rhs.value
    }

With the above `implicit class`es that all `extends Valueable`, you can safely do `expr.value` for any `Expression` value for which an implicit exists in scope and be done with the task at hand. Notice how Scala "executes" `value` on the different types (that's based upon using `implicit class`es for the types when `value` is needed).

    println(calculate(expr))  // prints 20

For what is worth, the `calculate` method belongs to a library that knows nothing about `Expression` type and you can't change it so `Expression`s would be worked with. That's exactly where typeclasses shines and are hard to beat.

This is the version of type classes pattern as described in the book [Programming Scala, 2nd Edition](http://shop.oreilly.com/product/0636920033073.do) in "Type Class Pattern" section, page 156.

### Final solution: `Value[T]` type class

You can still do better than that in Scala and that's what (*the real*) type class design pattern offers. This is the version of type class pattern as demonstrated in the video [Tutorial: Typeclasses in Scala with Dan Rosen](https://youtu.be/sVMES4RZF-8). You should really watch it.

Let's have a parameterized type `trait Value[T]` that's supposed to "bridge" the type hierarchies - `Expression` and `Valueable`:

    trait Value[T] {
      def value(t: T): Valueable
    }

The fictitious Calculator library has `object Calculator` with a single `def calculate(v: Valueable): Int` method:

    object Calculator {
      def calculate(v: Valueable): Int = v.value
    }

No `Expression`s, just `Valueable`s. That's where the type class pattern shines.

Create `object CalculatorEx` as follows:

    object CalculatorEx {
      def calculate[T : Value](t: T): Int =
        Calculator.calculate(implicitly[Value[T]].value(t))
    }

It says that for any type `T` there's an implicit conversion to a `Value[_]` type hierarchy using implicits (that are used in the method via [implicitly](http://www.scala-lang.org/api/current/index.html#scala.Predef$)).

All in all, you need to throw in three more implicits for `Number`, `Plus` and `Minus` so they can be seen as `Valueable` and participate in `def calculate(v: Valueable): Int`-based library:

    implicit val number2Value = new Value[Number] {
      def value(n: Number): Valueable = new Valueable {
        override def value: Int = n.value
      }
    }

    implicit val plus2Value = new Value[Plus] {
      def value(p: Plus): Valueable = new Valueable {
        override def value: Int = p.lhs.value + p.rhs.value
      }
    }

    implicit val minus2Value = new Value[Minus] {
      def value(m: Minus): Valueable = new Valueable {
        override def value: Int = m.lhs.value - m.rhs.value
      }
    }

These make it possible to calculate the value of the expression leveraging the fictitious Calculator library:

    println(CalculatorEx.calculate(expr))  // prints 20

That's it! You're done. If you've followed along closely and have developed the "framework" on your own, you should have a complete understanding of the type class design pattern in Scala. Without the type class pattern, blending the `Expression` type hierarchy with `Valueable` would not have been possible! And that was the goal of the exercise.

### Follow up - All `Valueable`

Think about using other types, say `Int`, with the fictitious Calculator library so the following is possible:

    println(CalculatorEx.calculate(1))  // prints 1

As the Scala compiler says:

> could not find implicit value for evidence parameter of type Value[Int]

All, we need to do to blend `Int`s as `Valueable`s is to have an implicit that does the conversion.

    implicit val int2Value = new Value[Int] {
      override def value(t: Int) = new Valueable {
        override def value: Int = t
      }
    }

And that's it!

As a final exercise would be to reuse implicits and create more complex ones to support "sum" types, like `Tuple`. I'm leaving it as a homework. Have fun!

Let me know how the homework and the whole blog post went out in the Comments section below. I'd appreciate any comments to improve upon.

## References

There are plenty of very good sources on the topic of type classes in general and in Scala, in particular, but the following have done wonders for me:

* [ad hoc polymorphism](http://en.wikipedia.org/wiki/Ad_hoc_polymorphism) article on Wikipedia
* [Polymorphism (computer science)](http://en.wikipedia.org/wiki/Polymorphism_(computer_science)) article on Wikipedia
* [Tutorial: Typeclasses in Scala with Dan Rosen](https://youtu.be/sVMES4RZF-8) video
* [Programming Scala, 2nd Edition](http://shop.oreilly.com/product/0636920033073.do) book]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ditching Guice's @Singleton in Favour of Scala's (Companion) Object]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/05/09/ditching-guices-at-singleton-in-favour-of-scalas-companion-object.html"/>
    <updated>2015-05-09T14:57:19+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/05/09/ditching-guices-at-singleton-in-favour-of-scalas-companion-object</id>
    <content type="html"><![CDATA[[Arek Komarzewski](https://twitter.com/akomarzewski) (a Scala developer in HCore) mentioned the following this Friday and made my day (and the whole week, too):

> I can now ditch Guice's @Singleton as I've got a trait and the companion object combo (thanks to Scala).

This time the blog post is without a complete working example. Not yet. It's to remind myself to prepare one (or be given one after the blog post is published -- whatever comes first). I just think it needs to be said aloud to be heard and think about.

<!-- more -->

And then, quite unexpectedly to me, [Play Framework](https://www.playframework.com/) - *"The High Velocity Web Framework For Java and Scala"* - that's the web framework supported by Typesafe in their [Reactive Platform](http://www.typesafe.com/products/typesafe-reactive-platform) has announced in [What's new in Play 2.4](https://www.playframework.com/documentation/2.4.x/Highlights24):

> In the Scala ecosystem, the approach to dependency injection is not generally agreed upon, with many competing compile time and runtime dependency injection approaches out there.

> Play’s philosophy in providing a dependency injection solution is to be unopinionated in what approaches we allow, but to be opinionated to the approach that we document and provide out of the box. For this reason, we have provided the following:

> An implementation that uses Guice out of the box

I'm very lucky to be able to pursue my understanding of Scala the programming language not only in my free time, but also in commercial projects as a full-time Scala developer and a technical leader (in [DeepSense.io](http://deepsense.io/)) as well as supporting companies making the most out of Scala and open source software (in [HCore](http://www.hcore.com/)) not to mention leading [Warsaw Scala Enthusiasts](http://www.meetup.com/WarszawScaLa/) in **Warsaw**, **Poland**. The technical part of my life simply can't be any better! I'm learning as well as teaching people using Scala as an object-oriented and functional programming language on JVM, and am also meeting up lots of Scala developers. I really wish I had more time to publish all the major breakthroughs in blog posts here.

Enough praising. On to real matters.

In [DeepSense.io](http://deepsense.io/) we're using [Guice](https://github.com/google/guice) as *a dependency framework*. It's also used in few other projects where Scala is used. That often leads to my questioning the need for Guice or any dependency injection framework since Scala the programming language itself offers enough features to let Guice leave.

I think there's the issue of how most Scala developers (I'm meeting) think -- they are former Java developers who see no reason to adapt to new approaches of tackling development problems. They simply have no more courage to dig deeper, and...let the past rest and welcome new solutions for the brighter future. And since Scala alone is still shaping itself and the Scala community is not clear on what to follow along and what to forget about, that all makes the leave-past-welcome-new approach harder.

Just this week I had a pleasure to meet two teams that both use Guice because nobody introduced viable alternatives (even when they use one already - Scala!). I don't consider myself skilled enough, either, since I'm pretty much a Guice newbie and hence just unable to counter the Guice's way of solving problems. It's that something inside me that is telling me that Guice is dragged along unnecessarily and gives a false perception of being the right fit to dependency injection-looking problems (even when it introduces more problems, mostly related to learning a yet another framework, than it solves). I'm glad Spring is far heavier as it would likely have found its way in the projects, too.

It has worked fine for Arek and I'm hoping it's going to work fine for me soon, too. I'll simply have to find it out and promote the right approach.
]]></content>
  </entry>
  
</feed>
