<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scala | Mastering FP and OO with Scala]]></title>
  <link href="http://blog.jaceklaskowski.pl/categories/scala/atom.xml" rel="self"/>
  <link href="http://blog.jaceklaskowski.pl/"/>
  <updated>2015-07-22T09:16:58+02:00</updated>
  <id>http://blog.jaceklaskowski.pl/</id>
  <author>
    <name><![CDATA[Jacek Laskowski]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Real-time Data Processing Using Apache Kafka and Spark Streaming (and Scala and Sbt)]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/07/20/real-time-data-processing-using-apache-kafka-and-spark-streaming.html"/>
    <updated>2015-07-20T21:02:39+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/07/20/real-time-data-processing-using-apache-kafka-and-spark-streaming</id>
    <content type="html"><![CDATA[It's been a while since I worked with [Spark Streaming](http://spark.apache.org/streaming/). It was back then when I was working for a pet project that ultimately ended up as a Typesafe Activator template [Spark Streaming with Scala and Akka](http://www.typesafe.com/activator/template/spark-streaming-scala-akka) to get people going with the technologies.

Time flies by very quickly and as [the other blog posts](http://blog.jaceklaskowski.pl/2015/07/14/apache-kafka-on-docker.html) [may have showed](http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html) I'm evaluating [Apache Kafka](http://kafka.apache.org/) as a potential messaging and integration platform for my future projects. A lot is happening in so called *big data space* and Apache Kafka fits the bill in many dataflows around me so well. I'm very glad it's mostly all [Scala](http://www.scala-lang.org/) which we all *love* and are spending our time with. Ain't we?

From [Spark Streaming documentation](http://spark.apache.org/docs/latest/streaming-programming-guide.html) (Kafka bolded on purpose):

> Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like **Kafka**, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window.

Since Apache Kafka aims at being **the central hub for real-time streams of data** (see [1.2 Use Cases](http://kafka.apache.org/documentation.html#uses) and [Putting Apache Kafka To Use: A Practical Guide to Building a Stream Data Platform (Part 1)](http://www.confluent.io/blog/stream-data-platform-1/)) I couldn't deny myself the simple pleasure of giving it a go.

Buckle up and ingest *some* data using Apache Kafka and Spark Streaming! You surely *will* love the infrastructure (if you haven't already). Be sure to type fast to see the potential of the platform at your fingertips.

<!-- more -->

## The project (using sbt)

Here is the sbt build file `build.sbt` for the project:

    val sparkVersion = "1.4.1"
    scalaVersion := "2.11.7"

    libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-streaming" % sparkVersion,
      "org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
    )

It uses the latest released versions of **Spark Streaming 1.4.1** and **Scala 2.11.7**.

## Setting up Kafka broker

It assumes you've already installed Apache Kafka. You may want to use Docker (see [Apache Kafka on Docker](http://blog.jaceklaskowski.pl/2015/07/14/apache-kafka-on-docker.html)) or [build Kafka from the sources](http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html). Whatever approach you choose, start Zookeeper and Kafka.

### Starting Zookeeper

I'm using the version of Kafka built from the sources and `./bin/zookeeper-server-start.sh` that comes with it.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/zookeeper-server-start.sh config/zookeeper.properties
    ...
    [2015-07-21 06:51:39,614] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)

### Starting Kafka broker

Once Zookeeper is up and running (in the above case, listening to the port 2181), run a Kafka broker.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-server-start.sh config/server.properties
    ...
    [2015-07-21 06:53:17,051] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT -> EndPoint(192.168.1.9,9092,PLAINTEXT) (kafka.utils.ZkUtils$)
    [2015-07-21 06:53:17,058] INFO [Kafka Server 0], started (kafka.server.KafkaServer)

There are merely two commands to boot the entire environment up and that's it.

### Creating topic - `spark-topic`

**A topic** is where you're going to send messages to and where Spark Streaming is consuming them from later on. It's the communication channel between producers and consumers and you've got to have one.

Create `spark-topic` topic. The name is arbitrary and pick whatever makes you happy, but use it consistently in the other places where the name's used.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic spark-topic
    Created topic "spark-topic".

You may want to check out the available topics.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --list --zookeeper localhost:2181
    spark-topic

You're now done with setting up Kafka for the demo.

### (optional) Sending to and receiving messages from Kafka

Apache Kafka comes with two shell scripts to send and receive messages from topics. They're `kafka-console-producer.sh` and `kafka-console-consumer.sh`, respectively. They both use the console (stdin) as the input and output.

Let's publish few messages to the `spark-topic` topic using `./bin/kafka-console-producer.sh`.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic spark-topic
    hello
    hi
    ^D

 You can keep the producer up in one terminal and use another terminal to consume the messages or just send a couple of messages and close the session. Kafka persists messages for a period of time.

Consuming messages is as simple as running `./bin/kafka-console-consumer.sh`. Mind the option `--zookeeper` to point to Zookeeper where Kafka stores its configuration and `--from-beginning` that tells Kafka to process all persisted messages.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic spark-topic --from-beginning
    hello
    hi
    ^DConsumed 2 messages

## Spark time!

Remember `build.sbt` above that sets the Scala/sbt project up with appropriate Scala version and Spark Streaming dependencies?

    val sparkVersion = "1.4.1"
    scalaVersion := "2.11.7"

    libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-streaming" % sparkVersion,
      "org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
    )

To learn a little about the integration between Spark Streaming and Apache Kafka you're going to use `sbt console` and type all the integration code line by line in the interactive console. You could have a simple Scala application, but I'm leaving it to you as an exercise.

You may want to read the scaladoc of [org.apache.spark.SparkConf](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf) and [org.apache.spark.streaming.StreamingContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext) to learn about their purpose in the sample.

    scala> import org.apache.spark.SparkConf
    import org.apache.spark.SparkConf

    scala> val conf = new SparkConf().setMaster("local[*]").setAppName("KafkaReceiver")
    conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@2f8bf5fc

    scala> import org.apache.spark.streaming.StreamingContext
    import org.apache.spark.streaming.StreamingContext

    scala> import org.apache.spark.streaming.Seconds
    import org.apache.spark.streaming.Seconds

    scala> val ssc = new StreamingContext(conf, Seconds(10))
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    15/07/21 09:08:39 INFO SparkContext: Running Spark version 1.4.1
    ...
    ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2ce5cc3

    scala> import org.apache.spark.streaming.kafka.KafkaUtils
    import org.apache.spark.streaming.kafka.KafkaUtils

    // Note the name of the topic in use - spark-topic
    scala> val kafkaStream = KafkaUtils.createStream(ssc, "localhost:2181","spark-streaming-consumer-group", Map("spark-topic" -> 5))
    kafkaStream: org.apache.spark.streaming.dstream.ReceiverInputDStream[(String, String)] = org.apache.spark.streaming.kafka.KafkaInputDStream@4ab601ac

    // The very complex BIG data analytics
    scala> kafkaStream.print()

    // Start the streaming context so Spark Streaming polls for messages
    scala> ssc.start
    15/07/21 09:11:31 INFO ReceiverTracker: ReceiverTracker started
    15/07/21 09:11:31 INFO ForEachDStream: metadataCleanupDelay = -1
    ...
    15/07/21 09:11:31 INFO StreamingContext: StreamingContext started

Spark Streaming is now connected to Apache Kafka and consumes messages every 10 seconds. Leave it running and switch to another terminal.

Open a terminal to run a Kafka producer. You may want to use `kafkacat` (highly recommended) or the producer that comes with Apache Kafka - `kafka-console-producer.sh`.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic spark-topic
    hey spark, how are you doing today?

Switch to the terminal with Spark Streaming running and see the message printed out.

    15/07/21 09:12:00 INFO DAGScheduler: ResultStage 1 (print at <console>:18) finished in 0.016 s
    15/07/21 09:12:00 INFO DAGScheduler: Job 1 finished: print at <console>:18, took 0.030530 s
    -------------------------------------------
    Time: 1437462720000 ms
    -------------------------------------------
    (null,hey spark, how are you doing today?)

    15/07/21 09:12:00 INFO JobScheduler: Finished job streaming job 1437462720000 ms.1 from job set of time 1437462720000 ms

Congratulations! You've earned the Spark Streaming and Apache Kafka integration badge! Close Spark Streaming's context using

    scala> ssc.stop

or simply press `Ctrl+C`. Shut down Apache Kafka and Zookeeper, too. Done.

## (bonus) Building Apache Spark from the sources

You could use the very latest version of Spark Streaming in which the latest and greatest development is going on and lives on [the master branch](https://github.com/apache/spark/commits/master).

Following the official documentation [Building with build/mvn](http://spark.apache.org/docs/latest/building-spark.html#building-with-buildmvn)[^1], execute the following two commands. Please note the versions in the build as it uses **Scala 2.11.7** and **Hadoop 2.7.1**.

    ➜  spark git:(master) ./dev/change-version-to-2.11.sh
    ➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Dscala-2.11 -DskipTests clean install
    ...
    [INFO] ------------------------------------------------------------------------
    [INFO] Reactor Summary:
    [INFO]
    [INFO] Spark Project Parent POM ........................... SUCCESS [  2.696 s]
    [INFO] Spark Project Launcher ............................. SUCCESS [  7.346 s]
    [INFO] Spark Project Networking ........................... SUCCESS [  6.577 s]
    [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  4.052 s]
    [INFO] Spark Project Unsafe ............................... SUCCESS [  3.591 s]
    [INFO] Spark Project Core ................................. SUCCESS [01:43 min]
    [INFO] Spark Project Bagel ................................ SUCCESS [  4.704 s]
    [INFO] Spark Project GraphX ............................... SUCCESS [ 10.014 s]
    [INFO] Spark Project Streaming ............................ SUCCESS [ 21.804 s]
    [INFO] Spark Project Catalyst ............................. SUCCESS [ 28.271 s]
    [INFO] Spark Project SQL .................................. SUCCESS [ 34.811 s]
    [INFO] Spark Project ML Library ........................... SUCCESS [ 45.990 s]
    [INFO] Spark Project Tools ................................ SUCCESS [  1.964 s]
    [INFO] Spark Project Hive ................................. SUCCESS [ 39.831 s]
    [INFO] Spark Project REPL ................................. SUCCESS [  3.219 s]
    [INFO] Spark Project YARN ................................. SUCCESS [  5.109 s]
    [INFO] Spark Project Assembly ............................. SUCCESS [01:10 min]
    [INFO] Spark Project External Twitter ..................... SUCCESS [  5.204 s]
    [INFO] Spark Project External Flume Sink .................. SUCCESS [  5.472 s]
    [INFO] Spark Project External Flume ....................... SUCCESS [  6.292 s]
    [INFO] Spark Project External Flume Assembly .............. SUCCESS [  2.345 s]
    [INFO] Spark Project External MQTT ........................ SUCCESS [  5.361 s]
    [INFO] Spark Project External ZeroMQ ...................... SUCCESS [  6.006 s]
    [INFO] Spark Project External Kafka ....................... SUCCESS [ 13.850 s]
    [INFO] Spark Project Examples ............................. SUCCESS [01:15 min]
    [INFO] Spark Project External Kafka Assembly .............. SUCCESS [ 27.458 s]
    [INFO] Spark Project YARN Shuffle Service ................. SUCCESS [  4.636 s]
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 09:06 min
    [INFO] Finished at: 2015-07-21T06:48:02+02:00
    [INFO] Final Memory: 76M/905M
    [INFO] ------------------------------------------------------------------------

The jars for the version are at your command in the Maven local repository. Switch the version of Spark Streaming to **1.5.0-SNAPSHOT** and start over. It is known to work.

## Summary

As it turns out setting up a working configuration of Apache Kafka and Spark Streaming is just few clicks away. There are a couple of places that need improvement, but what the article has showed could be a good starting point for other real-time big data analytics using **Apache Kafka** as **the central hub for real-time streams of data** that are then processed **using complex algorithms** in **Spark Streaming**.

Once the data's processed, Spark Streaming could be publishing results into yet another Kafka topic and/or store in Hadoop for later. It seems I've got a very powerful setup I'm not yet fully aware of where I should apply to.

Let me know what you think about the topic[^2] of the blog post in the [Comments](#disqus_thread) section below or contact me at jacek@japila.pl. Follow the author as [@jaceklaskowski](https://twitter.com/jaceklaskowski) on Twitter, too.

[^1]: *I really really wish the Apache Spark project hadn't migrated the build to Apache Maven from the top-notch interactive build tool - [sbt](http://www.scala-sbt.org/)*
[^2]: pun intended
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Publishing Events Using Custom Producer for Apache Kafka]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka.html"/>
    <updated>2015-07-19T23:04:21+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/07/19/publishing-events-using-custom-producer-for-apache-kafka</id>
    <content type="html"><![CDATA[I'm a [Scala](http://www.scala-lang.org/) proponent so when I found out that [the Apache Kafka team has decided to switch to using Java as the main language of the new client API](https://cwiki.apache.org/confluence/display/KAFKA/Client+Rewrite) it was beyond my imagination. [Akka](http://akka.io/docs/)'s fine with their Java/Scala APIs and so I can't believe [Apache Kafka](http://kafka.apache.org/) couldn't offer similar APIs, too. It's even more weird when one finds out that Apache Kafka itself is written in Scala. Why on earth did they decide to do the migration?!

In order to learn Kafka better, I developed a custom producer using the latest Kafka's Producer API in Scala. I built Kafka from the sources, and so I'm using the version **0.8.3-SNAPSHOT**. It was pretty surprising experience, esp. when I ran across  [java.util.concurrent.Future](http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Future.html) that seems so limited to what [scala.concurrent.Future](http://docs.scala-lang.org/overviews/core/futures.html) offers. No `map`, `flatMap` or such? So far I consider the switch to using Java for the Client API a big mistake.

Here comes the complete Kafka producer I've developed in Scala that's supposed to serve as a basis for my future development endeavours using the API in what's going to be in 0.8.3 release.

<!-- more -->

## Custom KafkaProducer in Scala

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">java.util.concurrent.Future</span>
</span><span class='line'>
</span><span class='line'><span class="k">import</span> <span class="nn">org.apache.kafka.clients.producer.RecordMetadata</span>
</span><span class='line'>
</span><span class='line'><span class="k">object</span> <span class="nc">KafkaProducer</span> <span class="k">extends</span> <span class="nc">App</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">topic</span> <span class="k">=</span> <span class="n">util</span><span class="o">.</span><span class="nc">Try</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="mi">0</span><span class="o">)).</span><span class="n">getOrElse</span><span class="o">(</span><span class="s">&quot;my-topic-test&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">println</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;Connecting to $topic&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">import</span> <span class="nn">org.apache.kafka.clients.producer.KafkaProducer</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">props</span> <span class="k">=</span> <span class="k">new</span> <span class="n">java</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Properties</span><span class="o">()</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;localhost:9092&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;client.id&quot;</span><span class="o">,</span> <span class="s">&quot;KafkaProducer&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;key.serializer&quot;</span><span class="o">,</span> <span class="s">&quot;org.apache.kafka.common.serialization.IntegerSerializer&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">props</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;value.serializer&quot;</span><span class="o">,</span> <span class="s">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">producer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">KafkaProducer</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">String</span><span class="o">](</span><span class="n">props</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">import</span> <span class="nn">org.apache.kafka.clients.producer.ProducerRecord</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">polish</span> <span class="k">=</span> <span class="n">java</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">format</span><span class="o">.</span><span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="n">ofPattern</span><span class="o">(</span><span class="s">&quot;dd.MM.yyyy H:mm:ss&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">now</span> <span class="k">=</span> <span class="n">java</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="nc">LocalDateTime</span><span class="o">.</span><span class="n">now</span><span class="o">().</span><span class="n">format</span><span class="o">(</span><span class="n">polish</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">record</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ProducerRecord</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">String</span><span class="o">](</span><span class="n">topic</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="n">s</span><span class="s">&quot;hello at $now&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">metaF</span><span class="k">:</span> <span class="kt">Future</span><span class="o">[</span><span class="kt">RecordMetadata</span><span class="o">]</span> <span class="k">=</span> <span class="n">producer</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">meta</span> <span class="k">=</span> <span class="n">metaF</span><span class="o">.</span><span class="n">get</span><span class="o">()</span> <span class="c1">// blocking!</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">msgLog</span> <span class="k">=</span>
</span><span class='line'>    <span class="n">s</span><span class="s">&quot;&quot;&quot;</span>
</span><span class='line'><span class="s">       |offset    = ${meta.offset()}</span>
</span><span class='line'><span class="s">       |partition = ${meta.partition()}</span>
</span><span class='line'><span class="s">       |topic     = ${meta.topic()}</span>
</span><span class='line'><span class="s">     &quot;&quot;&quot;</span><span class="o">.</span><span class="n">stripMargin</span>
</span><span class='line'>  <span class="n">println</span><span class="o">(</span><span class="n">msgLog</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">producer</span><span class="o">.</span><span class="n">close</span><span class="o">()</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>

## Building Kafka from the sources

In order to run the client you should build Kafka from the sources first and publish the jars to the local Maven repository. The reason to have the build is that the producer uses the very latest Kafka Producer API.

Building Kafka from the sources is as simple as executing `gradle -PscalaVersion=2.11.7 clean releaseTarGz` in the directory where you `git clone https://github.com/apache/kafka.git`d [the Kafka repo from GitHub](https://github.com/apache/kafka.git).

    ➜  kafka git:(trunk) gradle -PscalaVersion=2.11.7 clean releaseTarGz
    Building project 'core' with Scala version 2.11.7
    ...
    BUILD SUCCESSFUL

    Total time: 1 mins 23.233 secs

I was building the distro against **Scala 2.11.7**.

Once done, `core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz` is where you find the release package.

    ➜  kafka git:(trunk) ✗ ls -l core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz
    -rw-r--r--  1 jacek  staff  17006303 18 lip 13:19 core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz

Unpack it and `cd` to it.

    ➜  kafka git:(trunk) ✗ tar -zxf core/build/distributions/kafka_2.11-0.8.3-SNAPSHOT.tgz
    ➜  kafka git:(trunk) ✗ cd kafka_2.11-0.8.3-SNAPSHOT

## Zookeeper up and running

Running Zookeeper is the very first step you should do (as that's how Kafka maintains high-availability). Use `./bin/zookeeper-server-start.sh config/zookeeper.properties`:

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/zookeeper-server-start.sh config/zookeeper.properties
    [2015-07-20 00:17:08,134] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
    [2015-07-20 00:17:08,136] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
    [2015-07-20 00:17:08,136] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
    [2015-07-20 00:17:08,136] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
    [2015-07-20 00:17:08,136] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
    [2015-07-20 00:17:08,153] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
    [2015-07-20 00:17:08,154] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
    [2015-07-20 00:17:08,165] INFO Server environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT (org.apache.zookeeper.server.ZooKeeperServer)
    [2015-07-20 00:17:08,165] INFO Server environment:host.name=192.168.1.9 (org.apache.zookeeper.server.ZooKeeperServer)
    [2015-07-20 00:17:08,165] INFO Server environment:java.version=1.8.0_45 (org.apache.zookeeper.server.ZooKeeperServer)
    ...
    [2015-07-20 00:17:08,191] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)

## Kafka broker up and running

With Zookeeper up, start a Kafka broker using `./bin/kafka-server-start.sh config/server.properties` command:

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-server-start.sh config/server.properties
    [2015-07-20 00:18:33,574] INFO KafkaConfig values:
    	advertised.host.name = null
      ...
    	log.dir = /tmp/kafka-logs
      ...
    	zookeeper.connect = localhost:2181
    	zookeeper.sync.time.ms = 2000
    	port = 9092
      ...
    [2015-07-20 00:18:33,671] INFO starting (kafka.server.KafkaServer)
    [2015-07-20 00:18:33,673] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
    [2015-07-20 00:18:33,684] INFO Starting ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
    [2015-07-20 00:18:33,693] INFO Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT (org.apache.zookeeper.ZooKeeper)
    [2015-07-20 00:18:33,694] INFO Client environment:host.name=192.168.1.9 (org.apache.zookeeper.ZooKeeper)
    [2015-07-20 00:18:33,694] INFO Client environment:java.version=1.8.0_45 (org.apache.zookeeper.ZooKeeper)
    [2015-07-20 00:18:33,694] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
    ...
    [2015-07-20 00:18:34,414] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT -> EndPoint(192.168.1.9,9092,PLAINTEXT) (kafka.utils.ZkUtils$)
    [2015-07-20 00:18:34,419] INFO [Kafka Server 0], started (kafka.server.KafkaServer)

## Creating topic

You're now going to create `my-topic` topic where the custom producer is going to publish messages to. Of course, the name of the topic is arbitrary, but should match what the custom producer uses.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic my-topic-test
    Created topic "my-topic-test".

Check out the topics available using `./bin/kafka-topics.sh --list --zookeeper localhost:2181`. You should see one.

    ➜  kafka_2.11-0.8.3-SNAPSHOT git:(trunk) ✗ ./bin/kafka-topics.sh --list --zookeeper localhost:2181
    my-topic-test

## Scala/sbt project setup

Create a Scala/sbt project with the following `build.sbt`:

    val kafkaVersion = "0.8.3-SNAPSHOT"

    scalaVersion := "2.11.7"

    resolvers += Resolver.mavenLocal

    libraryDependencies += "org.apache.kafka" % "kafka-clients" % kafkaVersion

Use the following `project/build.properties`:

    sbt.version=0.13.9-RC3

## Sending messages using KafkaProducer - `sbt run`

With the setup, you should now be able to run `sbt run` to run the custom Scala producer for Kafka.

    [kafka-publisher]> run
    [info] Running KafkaProducer
    Connecting to my-topic-test
    SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
    SLF4J: Defaulting to no-operation (NOP) logger implementation
    SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

    offset    = 0
    partition = 0
    topic     = my-topic-test

    [success] Total time: 8 s, completed Jul 20, 2015 12:29:44 AM

Executing `sbt run` again should show a different offset for the sam partition and topic:

    [kafka-publisher]> run
    [info] Running KafkaProducer
    Connecting to my-topic-test
    SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
    SLF4J: Defaulting to no-operation (NOP) logger implementation
    SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

    offset    = 1
    partition = 0
    topic     = my-topic-test

    [success] Total time: 0 s, completed Jul 20, 2015 12:30:47 AM

## Using kafkacat as a Kafka message consumer

If you really would like to see the message on the other, receiving side, I strongly recommend using [kafkacat](https://github.com/edenhill/kafkacat) that, once installed, boils down to the following command:

    ➜  ~  kafkacat -C -b localhost:9092 -t my-topic-test
    hello at 20.07.2015 0:29:43
    hello at 20.07.2015 0:30:46

It will read all the messages already published to `my-topic-test` topic and print out others once they come.

That's it. Congratulations!

## Summary

The complete project is [on GitHub in kafka-producer repo](https://github.com/jaceklaskowski/kafka-producer).

You may also want to read [1.3 Quick Start](http://kafka.apache.org/documentation.html#quickstart) in the official documentation of Apache Kafka.

Let me know what you think about the topic[^1] of the blog post in the [Comments](#disqus_thread) section below or contact me at jacek@japila.pl. Follow the author as [@jaceklaskowski](https://twitter.com/jaceklaskowski) on Twitter, too.

[^1]: pun intended
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ad Hoc Polymorphism in Scala With Type Classes]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/05/15/ad-hoc-polymorphism-in-scala-with-type-classes.html"/>
    <updated>2015-05-15T22:21:20+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/05/15/ad-hoc-polymorphism-in-scala-with-type-classes</id>
    <content type="html"><![CDATA[My journey into the depths of [Scala](http://www.scala-lang.org/) is in full swing. Not only can I learn the theory (with the group of [Warsaw Scala Enthusiasts](http://warsawscala.pl)), but also apply it to commercial projects (with the Scala development teams of [DeepSense.io](http://deepsense.io/) and [HCore](http://www.hcore.com/)). Each day I feel I'm getting better at using **type system in Scala** in a more concious and (hopefully) efficient manner.

This time I sank into **type classes** that is a means of doing ** *ad hoc* polymorphism** in Scala.

From [*ad hoc* polymorphism](http://en.wikipedia.org/wiki/Ad_hoc_polymorphism) article on Wikipedia:

> In programming languages, **ad hoc polymorphism** is a kind of polymorphism in which polymorphic functions can be applied to arguments of different types, because a polymorphic function can denote a number of distinct and potentially heterogeneous implementations depending on the type of argument(s) to which it is applied.

The blog post presents a way to implement the type classes concept in Scala.

p.s. I'm yet to find out how much of it is [multimethods](http://clojure.org/multimethods) in [Clojure](http://clojure.org/) (that was once of much help to introduce me to **functional programming**).

<!-- more -->

## Theory

From the article [Polymorphism (computer science)](http://en.wikipedia.org/wiki/Polymorphism_(computer_science)) on Wikipedia, you can read about three different kinds of polymorphism:

* **subtyping**
* **parametric polymorphism**
* **ad hoc polymorphism**

The order does matter (and is different from the Wikipedia article's one) as I think that's exactly the order of when and how well developers master them (in any programming language).

I reckon the Scala community finds the first two quite often used in code that contributes to how well it's understood and applied (except [variance](http://en.wikipedia.org/wiki/Covariance_and_contravariance_%28computer_science%29)), and just the last one - ad hoc polymorphism - is proving a major hurdle for many, me including. It didn't click for me for a long time, either, only until I found the "venues" of very concise and comprehensible material (see [References](#references) section below).

> In programming languages and type theory, polymorphism is the provision of a single interface to entities of different types.

The difference between *subtyping* and *parametric polymorphism* and *ad hoc polymorphism* is that the type hierarchy is expressed explicitly using **extends** keyword in Scala (for subtyping) or type parameters (for parametric polymorphism) while ad hoc polymorphism bases itself on **implicit classes** to *mixin* the behaviour (using traits). And that's pretty much all, technically.

Let's see it in a Scala code.

## Practice

### Algebraic data types (raw data)

Let's assume you have the following type hierarchy (from [Tutorial: Typeclasses in Scala with Dan Rosen](https://youtu.be/sVMES4RZF-8)):

    sealed trait Expression
    case class Number(value: Int) extends Expression
    case class Plus(lhs: Expression, rhs: Expression) extends Expression
    case class Minus(lhs: Expression, rhs: Expression) extends Expression

No behaviour, but *algebraic data types* or (often called) *raw data*.

Think about how you'd go about evaluating expressions, i.e. `Plus(Minus(Number(5), Number(3)), Number(18))`, i.e. how to make the following application compile and print `20`?

    object Main extends App {
      val expr = Plus(Minus(Number(5), Number(3)), Number(18))
      println(...) // do something with expr so it prints 20
    }

The past me would change `sealed trait Expression` to include `def value: Int` and force the three case classes `Number`, `Plus` and `Minus` follow. I'd not be surprised if you thought exactly so. You should not, however.

Think about the case where you must **not** change them as they could be provided as a library or be part of the language or be licensed or...I simply ask you not to.

### objects to apply behaviour

Since you're in Scala, you could easily work it around with an object, say `object ExpressionEvaluator`, that would *pattern match* to types and do the heavy lifting, i.e. know what to do with each and every type:

    object ExpressionEvaluator {
      def value(expression: Expression): Int = expression match {
        case Number(value) => value
        case Plus(lhs, rhs) => value(lhs) + value(rhs)
        case Minus(lhs, rhs) => value(lhs) - value(rhs)
      }
    }

That's quite inefficient and cumbersome, though. You'd have to know about all of the implementations as well as about what to do for each. It's nearly impossible to have right, complete and still flexible.

With the above object, you'd write:

    object Main extends App {
      val expr = Plus(Minus(Number(5), Number(3)), Number(18))
      println(ExpressionEvaluator.value(expr)) // print the value
    }

### implicits in Scala

Let's do it the right way, so it's not `ExpressionEvaluator` to know what to do with every `Expression`, but expressions themselves do what they're supposed to do instead.

You may have heard about **implicits** in Scala. Perhaps, you may have used them, too. Think about a solution with implicit machinery so the following is possible:

    object Main extends App {
      val expr = Plus(Minus(Number(5), Number(3)), Number(18))
      println(expr.value) // print the value
    }

One way in Scala would be to apply **Pimp my Library** pattern leveraging `implicit class`es to add necessary methods as follows:

    implicit class ExpressionOps(e: Expression) {
      def value = ... // calculate the value
    }

And have a `implicit class` per `case class` and `sealed trait Expression`, too. The reason to have the implicits is to add a `value` method to every type, i.e. implicitly convert types without `value` to ones that have it.

    implicit class ExpressionOps(e: Expression) {
      def value: Int = e match {
        case n : Number => n.value
        case p : Plus => p.value
        case m : Minus => m.value
      }
    }

    implicit class PlusOps(p: Plus) {
      def value: Int = p.lhs.value + p.rhs.value
    }

    implicit class MinusOps(m: Minus) {
      def value: Int = m.lhs.value - m.rhs.value
    }

Note that since `Number` had `value` already, an implicit was not needed.

With the implicits in place, you can write:

    println(expr.value)  // prints 20

The implicit-based solution is far much more flexible because calculating a value is the responsibility of an implicit in scope -- a change in a single implicit, say `MinusOps`, would only change how `Minus.value` works (with no changes to the rest of the "framework").

You're halfway to typeclasses!

### Partial ad hoc polymorphism

Think about the case where you'd have a library to calculate a value of `Valueable` values (no pun intended). Say, you have such a library that offers the following "calculator":

    def calculate(v: Valueable) = ... // calculate the value of v

How would you go about converting `Expression`s to `Valueable`s so a value of `Expression` type would participate in the contract of `def calculate`? 

You're right that whenever type conversion is needed in Scala, implicits have their say -- you used them already to have `def value` for `Expression` type hierarchy. You're going to use them again with a very small change that has enormous impact. Doing so will introduce the type class design pattern.

The current solution relies on values with `def value` -- it's like having a library that uses structural typing in Scala that unfortunatelly uses reflection and hence is very costly performance-wise. Happily, you don't need structural types.

If you had a library that expects values of some type, say `trait Valueable`, to calculate a value or some other library to JSONify them (using some `trait Json`), the previous solutions would fall short -- you've merely met the requirement of being able to call a `value` method on values, but the values don't belong to a single type hierarchy of some `trait Valueable` that the library uses.

Assume you have a library that works with `trait Valueable`-only values and there's a `calculate` method to work with them as follows:

    trait Valueable {
      def value: Int
    }

    def calculate(v: Valueable) = v.value

Note that the library knows nothing about the `Expression` type hierarchy. The `Expression` type hierarchy could not even have existed at the time `Valueable` did.

A more flexible and efficient solution is to *somehow* meld the `Expression` type hierarchy with `Valueable` and create *is-a relationship*. No, no, you're not going to change the `Expression` trait in any way. You must **not** and you even **can't**, remember?

Welcome to **typeclasses** (also known as **type class design pattern**)!

In order to have `extends`-like semantic at runtime with no `extends Valueable` in the (closed and `sealed`) `Expression` type hierarchy you change `implicit class`es to have the common trait mixed in, instead.

    implicit class ExpressionOps(e: Expression) extends Valueable {
      def value = e match {
        case n: Number => n.value
        case p: Plus => p.value
        case m: Minus => m.value
      }
    }

    implicit class PlusOps(p: Plus) {
      def value: Int = p.lhs.value + p.rhs.value
    }

    implicit class MinusOps(m: Minus) {
      def value: Int = m.lhs.value - m.rhs.value
    }

With the above `implicit class`es that all `extends Valueable`, you can safely do `expr.value` for any `Expression` value for which an implicit exists in scope and be done with the task at hand. Notice how Scala "executes" `value` on the different types (that's based upon using `implicit class`es for the types when `value` is needed).

    println(calculate(expr))  // prints 20

For what is worth, the `calculate` method belongs to a library that knows nothing about `Expression` type and you can't change it so `Expression`s would be worked with. That's exactly where typeclasses shines and are hard to beat.

This is the version of type classes pattern as described in the book [Programming Scala, 2nd Edition](http://shop.oreilly.com/product/0636920033073.do) in "Type Class Pattern" section, page 156.

### Final solution: `Value[T]` type class

You can still do better than that in Scala and that's what (*the real*) type class design pattern offers. This is the version of type class pattern as demonstrated in the video [Tutorial: Typeclasses in Scala with Dan Rosen](https://youtu.be/sVMES4RZF-8). You should really watch it.

Let's have a parameterized type `trait Value[T]` that's supposed to "bridge" the type hierarchies - `Expression` and `Valueable`:

    trait Value[T] {
      def value(t: T): Valueable
    }

The fictitious Calculator library has `object Calculator` with a single `def calculate(v: Valueable): Int` method:

    object Calculator {
      def calculate(v: Valueable): Int = v.value
    }

No `Expression`s, just `Valueable`s. That's where the type class pattern shines.

Create `object CalculatorEx` as follows:

    object CalculatorEx {
      def calculate[T : Value](t: T): Int =
        Calculator.calculate(implicitly[Value[T]].value(t))
    }

It says that for any type `T` there's an implicit conversion to a `Value[_]` type hierarchy using implicits (that are used in the method via [implicitly](http://www.scala-lang.org/api/current/index.html#scala.Predef$)).

All in all, you need to throw in three more implicits for `Number`, `Plus` and `Minus` so they can be seen as `Valueable` and participate in `def calculate(v: Valueable): Int`-based library:

    implicit val number2Value = new Value[Number] {
      def value(n: Number): Valueable = new Valueable {
        override def value: Int = n.value
      }
    }

    implicit val plus2Value = new Value[Plus] {
      def value(p: Plus): Valueable = new Valueable {
        override def value: Int = p.lhs.value + p.rhs.value
      }
    }

    implicit val minus2Value = new Value[Minus] {
      def value(m: Minus): Valueable = new Valueable {
        override def value: Int = m.lhs.value - m.rhs.value
      }
    }

These make it possible to calculate the value of the expression leveraging the fictitious Calculator library:

    println(CalculatorEx.calculate(expr))  // prints 20

That's it! You're done. If you've followed along closely and have developed the "framework" on your own, you should have a complete understanding of the type class design pattern in Scala. Without the type class pattern, blending the `Expression` type hierarchy with `Valueable` would not have been possible! And that was the goal of the exercise.

### Follow up - All `Valueable`

Think about using other types, say `Int`, with the fictitious Calculator library so the following is possible:

    println(CalculatorEx.calculate(1))  // prints 1

As the Scala compiler says:

> could not find implicit value for evidence parameter of type Value[Int]

All, we need to do to blend `Int`s as `Valueable`s is to have an implicit that does the conversion.

    implicit val int2Value = new Value[Int] {
      override def value(t: Int) = new Valueable {
        override def value: Int = t
      }
    }

And that's it!

As a final exercise would be to reuse implicits and create more complex ones to support "sum" types, like `Tuple`. I'm leaving it as a homework. Have fun!

Let me know how the homework and the whole blog post went out in the Comments section below. I'd appreciate any comments to improve upon.

## References

There are plenty of very good sources on the topic of type classes in general and in Scala, in particular, but the following have done wonders for me:

* [ad hoc polymorphism](http://en.wikipedia.org/wiki/Ad_hoc_polymorphism) article on Wikipedia
* [Polymorphism (computer science)](http://en.wikipedia.org/wiki/Polymorphism_(computer_science)) article on Wikipedia
* [Tutorial: Typeclasses in Scala with Dan Rosen](https://youtu.be/sVMES4RZF-8) video
* [Programming Scala, 2nd Edition](http://shop.oreilly.com/product/0636920033073.do) book]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ditching Guice's @Singleton in Favour of Scala's (Companion) Object]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/05/09/ditching-guices-at-singleton-in-favour-of-scalas-companion-object.html"/>
    <updated>2015-05-09T14:57:19+02:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/05/09/ditching-guices-at-singleton-in-favour-of-scalas-companion-object</id>
    <content type="html"><![CDATA[[Arek Komarzewski](https://twitter.com/akomarzewski) (a Scala developer in HCore) mentioned the following this Friday and made my day (and the whole week, too):

> I can now ditch Guice's @Singleton as I've got a trait and the companion object combo (thanks to Scala).

This time the blog post is without a complete working example. Not yet. It's to remind myself to prepare one (or be given one after the blog post is published -- whatever comes first). I just think it needs to be said aloud to be heard and think about.

<!-- more -->

And then, quite unexpectedly to me, [Play Framework](https://www.playframework.com/) - *"The High Velocity Web Framework For Java and Scala"* - that's the web framework supported by Typesafe in their [Reactive Platform](http://www.typesafe.com/products/typesafe-reactive-platform) has announced in [What's new in Play 2.4](https://www.playframework.com/documentation/2.4.x/Highlights24):

> In the Scala ecosystem, the approach to dependency injection is not generally agreed upon, with many competing compile time and runtime dependency injection approaches out there.

> Play’s philosophy in providing a dependency injection solution is to be unopinionated in what approaches we allow, but to be opinionated to the approach that we document and provide out of the box. For this reason, we have provided the following:

> An implementation that uses Guice out of the box

I'm very lucky to be able to pursue my understanding of Scala the programming language not only in my free time, but also in commercial projects as a full-time Scala developer and a technical leader (in [DeepSense.io](http://deepsense.io/)) as well as supporting companies making the most out of Scala and open source software (in [HCore](http://www.hcore.com/)) not to mention leading [Warsaw Scala Enthusiasts](http://www.meetup.com/WarszawScaLa/) in **Warsaw**, **Poland**. The technical part of my life simply can't be any better! I'm learning as well as teaching people using Scala as an object-oriented and functional programming language on JVM, and am also meeting up lots of Scala developers. I really wish I had more time to publish all the major breakthroughs in blog posts here.

Enough praising. On to real matters.

In [DeepSense.io](http://deepsense.io/) we're using [Guice](https://github.com/google/guice) as *a dependency framework*. It's also used in few other projects where Scala is used. That often leads to my questioning the need for Guice or any dependency injection framework since Scala the programming language itself offers enough features to let Guice leave.

I think there's the issue of how most Scala developers (I'm meeting) think -- they are former Java developers who see no reason to adapt to new approaches of tackling development problems. They simply have no more courage to dig deeper, and...let the past rest and welcome new solutions for the brighter future. And since Scala alone is still shaping itself and the Scala community is not clear on what to follow along and what to forget about, that all makes the leave-past-welcome-new approach harder.

Just this week I had a pleasure to meet two teams that both use Guice because nobody introduced viable alternatives (even when they use one already - Scala!). I don't consider myself skilled enough, either, since I'm pretty much a Guice newbie and hence just unable to counter the Guice's way of solving problems. It's that something inside me that is telling me that Guice is dragged along unnecessarily and gives a false perception of being the right fit to dependency injection-looking problems (even when it introduces more problems, mostly related to learning a yet another framework, than it solves). I'm glad Spring is far heavier as it would likely have found its way in the projects, too.

It has worked fine for Arek and I'm hoping it's going to work fine for me soon, too. I'll simply have to find it out and promote the right approach.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Daily Routines to Learn Scala With IntelliJ IDEA]]></title>
    <link href="http://blog.jaceklaskowski.pl/2015/03/28/daily-routines-to-learn-scala-with-intellij-idea.html"/>
    <updated>2015-03-28T12:54:28+01:00</updated>
    <id>http://blog.jaceklaskowski.pl/2015/03/28/daily-routines-to-learn-scala-with-intellij-idea</id>
    <content type="html"><![CDATA[[<img class="left" src="/images/scala-idea-intellij-idea-14-1.png" title="IntelliJ IDEA 14.1 is Here!" >](http://blog.jetbrains.com/idea/2015/03/intellij-idea-14-1-is-here/) So, you've got a moment to learn [Scala](http://www.scala-lang.org/) and have [IntelliJ IDEA](https://www.jetbrains.com/idea/) with [Scala plugin](https://plugins.jetbrains.com/plugin/?id=1347) installed. Your wish is to *maximize* the mental outcome given the time at hand with *little to no effort* to set up a productive working environment. You may even think you may have gotten one, but, unless you're doing what I'm describing here, you're actually far from truly having it. I'm asking you to go *the extra mile*!

In this blog post I'm introducing you to two modes in the recently-shipped [IntelliJ IDEA 14.1](http://blog.jetbrains.com/idea/2015/03/intellij-idea-14-1-is-here/) -- **Full Screen** and **Distraction Free** modes -- and the few keystrokes I use in the development environment to have a comfortable place to learn Scala. I'm sure you'll have found few ideas to improve your way into your own personal Scala nirvana.

Let's go minimalistic, full screen, distraction-free, mouse- and touchpad-less!

You may find the blog post [What to Check Out in Scala Plugin 1.4.x for IntelliJ IDEA 14 & 14.1](http://blog.jetbrains.com/scala/2015/03/26/what-to-check-out-in-scala-plugin-1-4-x-for-intellij-idea-14-14-1/) helpful, too.

*Side note* It came as a complete surprise to me to have noticed that I've been writing the blog post exactly a month after the last one.

<!-- more -->

## Why I'm using IntelliJ IDEA to learn Scala?

I'm using IntelliJ IDEA daily.

I begin a day switching to the desktop where the IDE awaits my attention and keep it open (until a mandatory reboot following a system update). I was using other IDEs -- NetBeans IDE or Eclipse IDE -- in the past to develop applications in Java or Java EE, but things have changed since I switched focus on Scala entirely.

The reason for the switch was to master the Scala language not the other available IDEs, and given IntelliJ IDEA have always been receiving positive marks it's with me nowadays. When I need a full-blown IDE, it's IntelliJ IDEA with the Scala plugin. Period.

There's another tool that supports learning Scala beautifully -- **Scala REPL**. However it's often too rudimentary and limiting, for quick rendezvous I prefer it with Sublime Text 3 and sbt. For more advanced sessions nothing beats the beloved IDE - IntelliJ IDEA.

I think it was [Tomasz Nurkiewicz](http://www.nurkiewicz.com/) -- [an IntelliJ IDEA expert](http://blog.jetbrains.com/idea/2014/05/annotated-java-monthly-april-2014/) -- who first showed the beauty of using IntelliJ IDEA mouse- and touchpad-less. Thanks Tomek!

## Minimalistic workspace

[I remember the tweet from Adrian Gruntkowski](https://twitter.com/adrgrunt/status/552479034031239168) very well when the need to go minimalistic was first planted in my head. Adrian mentioned a user guide to set up a minimalistic workspace in IntelliJ IDEA (though it was for CursiveClojure) and the story began.

<img class="center" src="/images/scala-idea-tweet-minimalistic-workspace.png" title="Adrian mentions minimalistic workspace" >

It took me a while to get used to it, but it was worth it! I just needed a mixture of [Sublime Text 3](http://www.sublimetext.com/3) and IntelliJ IDEA as the Scala development environment and think I've found mine already.

## A day with Scala and IntelliJ IDEA

Be warned that I'm working on Mac OS X with `Mac OS X 10.5+` keymap so your milleage may vary.

The blog post assumes you've got a Scala project imported or created from scratch already. I don't bother explaining how to do it. In either case, IntelliJ IDEA should open with a Scala project so switching between files makes sense.

### Minimalistic IntelliJ IDEA

Follow [CursiveClojure UI](https://cursiveclojure.com/userguide/ui.html) to have a minimalistic, clutter-free workspace. It's a good start and boils down to turning off the toolbars, deselecting Toolbar and Navigation Bars in the View menu and finally disabling the Editor Tabs. That's a very good start.

Start by pressing `Cmd + Ctrl + F` to enter full screen.

You should have the IDE looked like as in the following screenshot.

<img class="center" src="/images/scala-idea-minimalistic-workspace.png" title="Minimalistic workspace following CursiveClojure UI" >

Press `Shift` key twice to open **Search everywhere** popup.

<img class="center" src="/images/scala-idea-search-action-popup.png" title="Search action popup" >

You may also want to use `Cmd + Shift + A` to search for actions only.

<img class="center" src="/images/scala-idea-search-actions-only.png" title="Search actions only popup" >

Type in **pre mod** or (better) **preMod** to execute **Enter Presentation Mode** action. In IDEA 14.1 there's far more productive mode - **Distraction Free Mode** so type in **distMod** instead.

<img class="center" src="/images/scala-idea-search-action-enter-distration-free-mode.png" title="Enter Distraction Free Mode" >

Either way -- Presentation or Distraction mode -- you've got a clean desk and should concentrate on Scala much easier (with the goodies of IntelliJ IDEA at your fingertips).

Following the advice in the empty workspace of IntelliJ IDEA, use `Cmd + E` to switch between files or `Cmd + Shift + E` to switch between files that were recently edited.

<img class="center" src="/images/scala-idea-recently-edited-files.png" title="Recently Edited Files popup" >

Use `Cmd + O` to open traits or classes. Use `Cmd + Shift + O` to open any file like `build.sbt` or project resources.

<img class="center" src="/images/scala-idea-enter-file-name-build-sbt.png" title="Enter file name popup" >

And the last but not least, install [BashSupport plugin](https://plugins.jetbrains.com/plugin/4230) to have a fully-supported terminal inside IntelliJ IDEA. Use `Alt + F12` to open a terminal session. I use it to have sbt shell open for `~ test` to have the tests executed every time the main and test sources change.

<img class="center" src="/images/scala-idea-terminal-window-sbt-test.png" title="Terminal window with sbt test" >

I use the terminal to open Scala REPL when I need to try out a new API.

<img class="center" src="/images/scala-idea-terminal-window-scala-repl.png" title="Terminal window with Scala REPL" >

There are also `Cmd + KeyUp` to select files from other directories in a project, or just switch to Project view with `Cmd + 1`. You could use `Alt + F1` to select the target to view the currently open file.

<img class="center" src="/images/scala-idea-change-directory-cmd-keyup.png" title="Cmd + KeyUp" >

## Summary

There are plenty of ways and tools to help you learn Scala in a pleasant environment. I've been using Sublime Text 3 and sbt for quite some time, and found myself very productive with this combo. In my case, though, *enough* turned out to be *lazy to learn more advanced developer tools*, i.e. IntelliJ IDEA.

Once I switched to IntelliJ IDEA and started using the features like Full Screen and Distraction Free modes with proper keystrokes, it became the development environment of choice to get full steam ahead into Scala.

[There ain't no such thing as a free lunch](http://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch) and it does take time to hop onto a new tooling and change habits. New things can often be getting into your way until you find them useful. As much as habits can help (speeding things up), they should not rule out others and mark them worse by default. As [Learn, Unlearn And Relearn: How To Stay Current And Get Ahead](http://www.forbes.com/sites/margiewarrell/2014/02/03/learn-unlearn-and-relearn/) says:

> Whatever the reasons, once the basics are covered, many people tend to stick with what they know and avoid situations or challenges where they may mess up or be forced to learn something new, thus creating a safe, secure and comfortable (and confining) world for themselves.

Give the tips from the blog post a try and a month later you will have found they're as pleasant as yours. Or bring you even more joy! Let me know how it's worked out in the Comments section below.
]]></content>
  </entry>
  
</feed>
